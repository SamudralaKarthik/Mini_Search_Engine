next prev does Tokenizing Text, Sentence, Words Works? Natural Language Processing (NLP) area computer science, along with artificial intelligence, information engineering, human-computer interaction. focus this field computers programmed processing analysing huge quantities data from natural languages. It's easy since process understanding reading languages much more intricate than appears first. Tokenization process breaking text string into array tokens. users think tokens distinct parts like word token sentence, while sentence token within form paragraph. Elements this Tutorial: Text into sentences. Tokenization Words into sentences tokenization Sentences using tokenization regular expressions Sentence Tokenization Sentence Tokenization splitting sentences paragraph Code from nltk.tokenize import sent_tokenize ST text1 "Hello everyone. Welcome Javatpoint. studying Tutorial" ST(text1) Output: ['Hello everyone.', 'Welcome Javatpoint.', studying Tutorial'] "sent_tokenize" Works? sent_tokenize function PunktSentenceTokenizer instance from nltk.tokenize.punkt module, which trained already therefore very well known marking beginning sentence characters punctuation. PunktSentenceTokenizer PunktSentenceTokenizer mostly used small data, cause it's hard deal with massive amount data. Code import nltk.data Here, will load PunktSentenceTokenizer using English pickle file tokenizer1 ND.load('tokenizers/punkt/english.pickle') tokenizer1.tokenize(text1) Output: ['Hello everyone.', 'Welcome Javatpoint.', studying Tutorial'] Tokenize sentence different language tokenize sentence various languages using pickle file other language than English. Code import nltk.data spanish_tokenizer1 ND.load('tokenizers/punkt/spanish.pickle') text1 'Hola todos. Bienvenido JavatPoint. Estamos estudiando Tutorial' spanish_tokenizer1.tokenize(text1) Output: ['Hola todos.', 'Bienvenido JavatPoint.', 'Estamos estudiando Tutorial'] Word Tokenization Word Tokenization used splitting words sentence. Code from nltk.tokenize import word_tokenize WT text1 "Hello everyone. Welcome Javatpoint. studying Tutorial" WT(text1) Output: ['Hello', 'everyone', '.', 'Welcome', 'to', 'Javatpoint', '.', 'We', 'are', 'studying', 'NLP', 'Tutorial'] "word_tokenize" Works? word_tokenize() function basically wrapper function which used calling tokenize() function that instance TreebankWordTokenizer class. Using TreebankWordTokenizer Code from nltk.tokenize import TreebankWordTokenizer TWT tokenizer1 TWT() tokenizer1.tokenize(text1) Output: ['Hello', 'everyone.', 'Welcome', 'to', 'Javatpoint.', 'We', 'are', 'studying', 'NLP', 'Tutorial'] These tokenizers operate separating words punctuation spaces. This allows user choose deal with punctuations during processing. outputs code above, doesn't eliminate punctuation. PunktWordTokenizer PunktWordTokenizer does separates punctuation from words. Code from nltk.tokenize import PunktWordTokenizer PWT tokenizer1 PWT() tokenizer1.tokenize("Let's it's working.") Output: ['Let', "'s", 'see', 'how', 'it', "'s", 'working', '.'] WordPunctTokenizer WordPunctTokenizer used separating punctuation from words. Code from nltk.tokenize import WordPunctTokenizer WPT tokenizer1 WPT() tokenizer1.tokenize("Hello everyone. Welcome Javatpoint. studying Tutorial") Output: ['Hello', 'everyone', '.', 'Welcome', 'to', 'Javatpoint', '.', 'We', 'are', 'studying', 'NLP', 'Tutorial'] Using Regular Expression Code from nltk.tokenize import RegexpTokenizer RT tokenizer1 RT("[\w']+") text1 ("Hello everyone. Welcome Javatpoint. studying Tutorial") tokenizer1.tokenize(text1) Output: ['Hello', 'everyone', 'Welcome', 'to', 'Javatpoint', 'We', 'are', 'studying', 'NLP', 'Tutorial'] Conclusion: this tutorial, have discussed different functions modules NLTK library tokenizing sentence words English well different languages using pickle method. Next TopicHow Import Datasets using sklearn PyBrain prev next