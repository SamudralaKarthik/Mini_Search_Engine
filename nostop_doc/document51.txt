next prev PySpark MLlib Machine Learning technique data analysis that combines data with statistical tools predict output. This prediction used various corporate industries make favorable decision. PySpark provides work with Machine learning called mllib. PySpark's mllib supports various machine learning algorithms like classification, regression clustering, collaborative filtering, dimensionality reduction well underlying optimization primitives. Various machine learning concepts given below: classification pyspark.mllib library supports several classification methods such binary classification, multiclass classification, regression analysis. object belong different class. objective classification differentiate data based information. Random Forest, Naive Bayes, Decision Tree most useful algorithms classification. clustering Clustering unsupervised machine learning problem. used when know classify data; require algorithm find patterns classify data accordingly. popular clustering algorithms K-means clustering, Gaussian mixture model, Hierarchical clustering. means frequent pattern matching, which used mining various items, itemsets, subsequences, other substructure. mostly used large-scale datasets. linalg mllib.linalg utilities used linear algebra. recommendation used define relevant data making recommendation. capable predicting future preference recommending items. example, Online entertainment platform Netflix huge collection movies, sometimes people face difficulty selecting favorite items. This field where recommendation plays important role. mllib regression regression used find relationship dependencies between variables. finds correlation between each feature data predicts future values. mllib package supports many other algorithms, classes, functions. Here will understand basic concept pyspak.mllib. MLlib Features PySpark mllib useful iterative algorithms. features following: Extraction: extracts features from "row" data. Transformation: used scaling, converting, modifying features. Selection: Selecting useful subset from larger features. Locality Sensitive Hashing: combines aspects feature transformation with other algorithms. Let's have look essential libraries PySpark MLlib. MLlib Linear Regression Linear regression used find relationship dependencies between variables. Consider following code: frompyspark.sql import SparkSession spark SparkSession.builder.appName('Customer').getOrCreate() frompyspark.ml.regression import LinearRegression dataset spark.read.csv(r'C:\Users\DEVANSH SHARMA\Ecommerce-Customers.csv') dataset.show(10) Output: +--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+ _c0| _c1| _c2| _c3| _c4| _c5| _c6| _c7| +--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+ Email| Address| Avatar|Avg Session Length| Time App| Time Website|Length Membership|Yearly Amount Spent| |[email protected]|835 Frank TunnelW...| Violet| 34.49726772511229| 12.65565114916675| 39.57766801952616| 4.0826206329529615| 587.9510539684005| [email protected]|4547 Archer Commo...| DarkGreen| 31.92627202636016|11.109460728682564|37.268958868297744| 2.66403418213262| 392.2049334443264| [email protected]|24645 Valerie Uni...| Bisque|33.000914755642675|11.330278057777512|37.110597442120856| 4.104543202376424| 487.54750486747207| |[email protected]|1414 David Throug...| SaddleBrown| 34.30555662975554|13.717513665142507| 36.72128267790313| 3.120178782748092| 581.8523440352177| |[email protected]|14023 Rodriguez P...|MediumAquaMarine| 33.33067252364639|12.795188551078114| 37.53665330059473| 4.446308318351434| 599.4060920457634| |[email protected]|645 Martha Park A...| FloralWhite|33.871037879341976|12.026925339755056| 34.47687762925054| 5.493507201364199| 637.102447915074| |[email protected]|68388 Reyes Light...| DarkSlateBlue| 32.02159550138701|11.366348309710526| 36.68377615286961| 4.685017246570912| 521.5721747578274| [email protected]|Unit 6538 898...| Aqua|32.739142938380326| 12.35195897300293| 37.37335885854755| 4.4342734348999375| 549.9041461052942| |[email protected]|860 KeyWest D...| Salmon| 33.98777289568564|13.386235275676436|37.534497341555735| 3.2734335777477144| 570.2004089636196| +--------------------+--------------------+----------------+------------------+------------------+------------------+--------------------+-------------------+ only showing rows following code, importing VectorAssembler library create column Independent feature: frompyspark.ml.linalg import Vectors frompyspark.ml.feature import VectorAssembler featureassembler VectorAssembler(inputCols ["Avg Session Length","Time App","Time Website"],outputCol "Independent Features") output featureassembler.transform(dataset) output.show() Output: +------------------+ Independent Feature +------------------+ |34.49726772511229 |31.92627202636016 |33.000914755642675| |34.30555662975554 |33.33067252364639 |33.871037879341976| |32.02159550138701 |32.739142938380326| |33.98777289568564 +------------------+ featureassembler.transform(dataset) finlized_data z.select("Indepenent feature", "Yearly Amount Spent",) z.show() Output: +--------------------++-------------------+ |Independent Feature Yearly Amount Spent| +--------------------++-------------------+ |34.49726772511229 587.9510539684005 |31.92627202636016 392.2049334443264 |33.000914755642675 487.5475048674720 |34.30555662975554 581.8523440352177 |33.33067252364639 599.4060920457634 |33.871037879341976 637.102447915074 |32.02159550138701 521.5721747578274 |32.739142938380326 549.9041461052942 |33.98777289568564 570.2004089636196 +--------------------++-------------------+ PySpark provides LinearRegression() function find prediction given dataset. syntax given below: regressor LinearRegression(featureCol 'column_name1', labelCol 'column_name2 MLlib Mean Cluster Mean cluster algorithm most popular commonly used algorithms. used cluster data points into predefined number clusters. below example showing MLlib K-Means Cluster library: from pyspark.ml.clustering import KMeans from pyspark.ml.evaluation import ClusteringEvaluator Loads data. dataset spark.read.format("libsvm").load(r"C:\Users\DEVANSH SHARMA\Iris.csv") Trains k-means model. kmeans KMeans().setK(2).setSeed(1) model kmeans.fit(dataset) Make predictions predictions model.transform(dataset) Evaluate clustering computing Silhouette score evaluator ClusteringEvaluator() silhouette evaluator.evaluate(predictions) print("Silhouette with squared euclidean distance str(silhouette)) Shows result. centers model.clusterCenters() print("Cluster Centers: center centers: print(center) Parameters PySpark MLlib important parameters PySpark MLlib given below: Ratings RDD Ratings (userID, productID, rating) tuple. Rank represents Rank computed feature matrices (number features). Iterations represents number iterations ALS. (default: Lambda Regularization parameter. (default 0.01) Blocks used parallelize computation some number blocks. Collaborative Filtering (mllib.recommendation) Collaborative filtering technique that generally used recommender system. This technique focused filling missing entries user-item. Association matrix spark.ml currently supports model-based collaborative filtering. collaborative filtering, users products described small hidden factors that used predict missing entries. Scaling regularization parameter regularization parameter regParam scaled solve least-squares problem. least-square problem occurs when number ratings user-generated updating user factors, number ratings product received updating product factors. Cold-start strategy Model (Alternative Least Square Model) used prediction while making common prediction problem. problem encountered when user items test dataset occurred that present during training model. occur scenarios which given below: prediction, model trained users items that have rating history called cold-start strategy). data splitted between training evaluation sets during cross-validation. widespread encounter users items evaluation that training set. Let's consider following example, where load ratings data from MovieLens dataset. Each containing user, movie, rating timestamp. #importing libraries frompyspark.ml.evaluation import RegressionEvaluator frompyspark.ml.recommendation import frompyspark.sql import no_of_lines spark.read.text(r"C:\Users\DEVANSH SHARMA\MovieLens.csv").rdd no_of_parts no_of_lines.map(lambda row: row.value.split("::")) ratingsRDD no_of_lines.map(lambda Row(userId=int(p[0]), movieId=int(p[1]), rating=float(p[2]), timestamp=long(p[3]))) ratings spark.createDataFrame(ratingsRDD) (training, test) ratings.randomSplit([0.8, 0.2]) Develop recommendation model using training data Note cold start strategy make sure that don't evaluation metrics. als ALS(maxIter=5, regParam=0.01, userCol="userId", itemCol="movieId", ratingCol="rating", coldStartStrategy="drop") model als.fit(training) Calculate model computing RMSE test data predictions model.transform(test) evaluator RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction") rmse evaluator.evaluate(predictions) print("Root-mean-square error str(rmse)) Evaluate movie recommendations each user userRecs model.recommendForAllUsers(10) Evaluate user recommendations each movie movieRecs model.recommendForAllItems(10) Evaluate movie recommendations specified users users ratings.select(als.getUserCol()).distinct().limit(3) userSubsetRecs model.recommendForUserSubset(users, Evalute user recommendations specified movies movies ratings.select(als.getItemCol()).distinct().limit(3) movieSubSetRecs model.recommendForItemSubset(movies, Next TopicPython Decorator prev next