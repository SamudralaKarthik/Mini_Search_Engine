next prev Sklearn Regression Models Machine learning utilized tackle regression question using different algorithms perform regression analysis: logistic regression linear regression. These most widely used regression approaches. Regression analysis approaches machine learning come many algorithms, their depends type data analysis aim. This tutorial will describe various machine learning regression models circumstances which apply each. This tutorial will undoubtedly grasping regression modelling concept novice machine learning. Regression Models supervised machine learning paradigm, system taught results. model's input output variables known both used training algorithm. algorithm true values training phase, machine learning model trained lower prediction error. main supervised learning algorithm categories regression (for continuous target variable) classification (for discrete target variable). predictive modelling method called regression analysis examines relationship between target dependent feature independent feature dataset. Various regression models used depending dependent independent features exhibit linear non-linear association between another target feature continuous values. Regression analysis frequently used identify cause effect relationships, forecast trends, time series forecasting analysis, predictor strength. Regression gives result continuous data. With help this technique, which centered characteristics, predict patterns training data. output real numerical number; however, doesn't fall into class category. instance, estimating property values depends details like home's size, locality, development ratio, among many other things. Types Regression Analysis Techniques Regression analysis approaches come wide variety, factors, discussed above, will determine which technique used. Examples these factors regression line pattern number independent features. many regression approaches listed below: Linear Regression Logistic Regression Ridge Regression Lasso Regression Polynomial Regression Bayesian Linear Regression Decision Tree Regression Support Vector Regression Gradient Boosting Regression Linear Regression machine learning method called linear regression establishes linear relationship among multiple independent features particular dependent feature forecast dependent feature's best value predicting linear equation's variables' coefficients. simple linear regression model calculates best fitting line dependent feature single independent feature (x). accompanying straight-line equation defines regression coefficient. indicates much anticipate alter value changes. regression model determines optimal intercept value regression coefficients minimize error (e). algorithm finds optimum values parameters reducing error between actual values target variable values predicted model. ordinary least squares approach used calculate error. accommodate many input variables present dataset. Code Python program perform linear regression (Simple Regression Multiple Regression) Importing required libraries import numpy import pandas from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn.datasets import load_diabetes from sklearn.linear_model import LinearRegression Loading diabetes dataset diabetes load_diabetes() Checking target feature continuous data print(diabetes.target[:10]) Creating dataframe dataset pd.DataFrame(data diabetes.data, columns diabetes.feature_names) Adding target variable dataset dataset["target"] diabetes.target Separating dependent independent features dataset.iloc[:, :-1].values dataset.iloc[:, -1].values Separating data training testing model X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.3, random_state Creating object Linear Regression model class l_reg LinearRegression() Fitting training data linear regression model Creating Simple Linear Regression model using only independent feature training dataset l_reg.fit(X_train[:, 2:3], Y_train) Printing R-square simple linear regression model passing unseen testing data score l_reg.score( X_test[:, 2:3], Y_test print("The r-square score Simple Linear Regression model: score) Creating Multiple Linear Regression model using independent features l_reg.fit(X_train, Y_train) Finding predicted values Y_pred l_reg.predict(X_test) Creating dataframe coefficient values independent features coef pd.DataFrame(data dataset.iloc[:,:-1].columns, columns ["Features"]) coef["Coefficients"] l_reg.coef_ coef.loc[0] ["Intercept", l_reg.intercept_] coef Output: [151. 75. 141. 206. 135. 97. 138. 63. 110. 310.] r-square score Simple Linear Regression model: 0.3195508704110651 	Features	Coefficients 0	Intercept	150.559849 1	sex	-207.159699 2	bmi	545.523615 3	bp	282.960253 4	s1	-1195.950976 5	s2	580.404692 6	s3	404.413845 7	s4	431.717208 8	s5	871.098497 9	s6	118.346669 Logistic Regression Logistic Regression another type regression modelling technique employed dependent feature discrete, such true false, etc. result, dependent variable only possible values take, sigmoid curve depicts association between target input variables. logistic regression algorithm uses logit function quantify relationship between target input variables. Code Python program create regression model using Logistic Regression algorithm Importing required modules from sklearn.datasets import load_iris from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression Loading iris dataset load_iris(return_X_y True) Separating data training testing model X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.4, random_state Creating object Logistic Regression model class log_reg LogisticRegression(random_state Fitting training data logistic regression model log_reg.fit(X_train, Y_train) Predicting values unseen data Y_pred log_reg.predict(X_test) Computing accuracy score Logistic Regression model scores accuracy_score(Y_test, Y_pred) print(scores) Output: Ridge Regression When independent features have high correlation value, Ridge regression algorithm predicts target variable. This because, non-collinear variables, least square estimates unbiased answer. However, there bias component collinearity strong. result, bias grid induced equation Ridge Regression. This powerful regression method makes constructed model much less likely overfit. Code Python program create regression model using Ridge Regression algorithm Importing required modules from sklearn.datasets import load_diabetes from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge Loading iris dataset load_diabetes(return_X_y True) Separating data training testing model X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.4, random_state Creating Logistic Regression model class object. model will using solver 'svd.' r_reg Ridge(solver 'svd', random_state Creating another model with solver 'lsqr.' This fastest solver based least square routine sklearn r_reg1 Ridge(solver 'lsqr', random_state Fitting training data Ridge regression model r_reg.fit(X_train, Y_train) r_reg1.fit(X_train, Y_train) Predicting values unseen data Y_pred r_reg.predict(X_test) Y_pred1 r_reg1.predict(X_test) Computing accuracy score both models print('Accuracy score solver "auto": r2_score(Y_test, r_reg.predict(X_test).round(5))) print('Accuracy score solver "lsqr": r2_score(Y_test, r_reg1.predict(X_test).round(5))) Output: Accuracy score solver "auto": 0.40731258229249656 Accuracy score solver "lsqr": 0.4073540170017558 Lasso Regression regression model applied learning algorithms that integrate feature selection, normalization procedures called Lasso Regression. absolute value regression coefficient considered. outcome, unlike Ridge Regression, independent features' coefficient value close zero. Lasso Regression involves feature selection. This process permits selecting group variables from given dataset, which will cause more change model than other variables. Lasso Regression, other features zero except features required make good predictions. This step helps keep model from overfitting. When collinearity value dataset's independent factors severe, lasso regression only chooses variable reduces coefficients other variables zero. Code Python program create regression model using Lasso Regression algorithm Importing required modules from sklearn.datasets import load_diabetes from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import Lasso Loading iris dataset load_diabetes(return_X_y True) Segregating training testing data X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.3, random_state Creating object Lasso Regression model class l_reg Lasso(random_state Fitting training data Lasso regression model l_reg.fit(X_train, Y_train) Predicting values unseen data Y_pred l_reg.predict(X_test) Computing accuracy score both models print('The accuracy score model r2_score(Y_test, l_reg.predict(X_test).round(5))) Output: accuracy score model 0.40131502999775714 Polynomial Regression Another regression analysis method used learning algorithms polynomial regression. This method similar multiple linear regression with some minor adjustments. degree polynomial regression defines link between independent dependent features, predictor, uses linear model regression; scale features using polynomial scaler function sklearn. algorithm polynomial regression, like linear regression, uses Ordinary Least Squares method compare errors lines. Instead being straight line polynomial regression, best-fitting line curve depending power value crosses data points. While trying achieve lowest value OLS equation find best-fitting curve, polynomial regression model prone overfitting. Evaluating different regression curves, end, advised because extrapolating higher polynomials produce results. dataset available here- https://github.com/content-anu/dataset-polynomial-regression Code Python program perform Polynomial Regression using sklearn #importing required libraries import numpy import pandas from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression import matplotlib.pyplot %matplotlib inline #importing uploaded dataset data pd.read_csv('Position_Salaries.csv') Seperating independent dependent features dataset data.iloc[:,1:2].values data.iloc[:,2].values Creating instance Polynomial Scaler poly_reg PolynomialFeatures(degree Fitting transforming dataset Polynomial Scaler X_polynomial poly_reg.fit_transform(X) Creating Linear regression model instance lreg LinearRegression() lreg.fit(X_polynomial, #Visualisng Polynomial regression model plt.scatter(X, color 'red') plt.plot(X, lreg.predict(poly_reg.fit_transform(X)), color 'blue') plt.title('Polynomial Regression Model') plt.xlabel('Position Levels') plt.ylabel('Salary') plt.show() Using higher degree polynomial show overfitting poly_reg1 PolynomialFeatures(degree X_polynomial1 poly_reg1.fit_transform(X) lreg1 LinearRegression() lreg1.fit(X_polynomial1, plt.scatter(X, color 'red') plt.plot(X, lreg1.predict(X_polynomial1), color 'blue') plt.title('Overfitted Polynomial Regression Model') plt.show() Output: Bayesian Linear Regression regression models used machine learning, Bayesian Regression, calculates magnitude regression coefficients using Bayes theorem. Rather than locating least squares, this regression approach determines variables' posterior distribution. Like linear regression ridge regression methods, Bayesian linear regression method much more robust than simple linear regression. Code Python program perform Bayesian Regression Importing modules that required from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score from sklearn.linear_model import BayesianRidge Loading Boston dataset load_boston(return_X_y True) Splitting training testing datasets X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.3, random_state Creating instance model training br BayesianRidge() br.fit(X_train, Y_train) Predicting values unseen data, i.e., testing data Y_pred br.predict(X_test) Computing R-square score model print(f"The score model {r2_score(Y_test, Y_pred)}") Output: score model 0.6312926702997255 Elastic Regression Elastic regression regularised linear regression approach. inserts costs loss function while training linearly combining both. blends lasso ridge regression giving each penalty proper weight, enhancing predictive accuracy. Alpha Lambda configurable hyperparameters elastic nets. Lambda controls proportion weighted total penalties that determines model's effectiveness. contrast, alpha parameter controls weight assigned each penalty individually. Code Python program perform Elastic Regression algorithm Importing required libraries from sklearn.linear_model import ElasticNet import numpy Constructing Elastic regression model having hyperparameter value alpha ElasticNet(alpha 0.1) Preparing input data np.array([[2, 4]]) Preparing target variables Fitting data model el.fit(X, Printing coefficients model print(el.coef_) Printing intercept best fitting line print(el.intercept_) Prediting value sample data print(el.predict([[4, 4]])) print(el.predict([[0, 0]])) Output: [0.66666667 1.79069767] 2.461240310077521 [12.29069767] [2.46124031] Decision Tree Regression possible represent judgments their probable consequences, covering outcomes, input penalties, utility, using decision-making tool called decision tree. supervised learning methods group includes decision-tree strategy, functions with output results that categorical continuous values. Decision Tree Regression: Decision tree regression develops model forecast future data provide relevant continuous output observing attributes item providing these attributes input algorithm. Continuous output denotes absence discrete outcome, i.e., output that only expressed discrete, well-known collection figures values. Code Python program show employ Decision Tree Regression Importing required modules import numpy from sklearn.datasets import load_diabetes from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import train_test_split, cross_val_score Loading diabetes dataset load_diabetes( return_X_y True Separating whole dataset datasets train test model X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.3, random_state Creating object Decision Tree Regressor class dtr DecisionTreeRegressor(random_state dtr.fit(X_train, Y_train) Computing accuracy score Decision Tree model using in-built cross_val_score method scores cross_val_score(dtr, Printing scores print("Accuracy scores splits: scores) print("The mean accuracy score model: np.mean(scores)) Output: Accuracy scores splits: [-1.02298964 0.05276312 -0.74850198 -0.07277615 0.47050685 0.3024374 -1.31209035 -0.26358347 0.43173477 -0.19109809 -0.56778646 -0.61486148 -0.11295867 0.0408493 -0.26464188] mean accuracy score model: -0.25819978131125926 Support Vector Regression Support Vector Regression (SVR) unique compared other regression models. employs Support Vector Machine (SVM, classification approach) forecast continuous parameter. Support Vector Regression seeks best line inside predetermined threshold deviation value, opposed conventional linear regression models that minimize discrepancy between estimated actual value. this respect, categorizes forecasting lines into types: those that cross error limit region divided parallel lines) those that not. When determining whether discrepancy between estimated value true value beyond error threshold, lines that cross error border considered (epsilon). lines crossing error threshold added group potential support vectors forecasting unknown value. better understand this idea look illustration below. kernel parameter most crucial could Gaussian kernel, Polynomial kernel, Linear kernel. choose Polynomial Gaussian kernel model have non-linear property dataset, this case, will select RBF (Gaussian type) kernel. Code Python program apply Support Vector Regression Importing required modules import numpy import matplotlib.pyplot import pandas from sklearn.preprocessing import StandardScaler from sklearn.svm import Importing dataset data pd.read_csv('Position_Salaries.csv') Separating target independent variables data.iloc[:,1:2].values.astype(float) data.iloc[:,2:3].values.astype(float) Performing feature scaling scaler StandardScaler() scaler1 StandardScaler() scaler.fit_transform(X) scaler1.fit_transform(y) Creating object Support Vector Regression class fitting dataset construct model SVR(kernel 'rbf') reg.fit(X, Predicting value given initial condition Y_pred reg.predict([[7.5]]) print(Y_pred) Visualising model using matplotlib plots plt.scatter(X, color 'red') plt.plot(X, reg.predict(X), color 'blue') plt.title('Support Vector Regression Model') plt.xlabel('Position levels') plt.ylabel('Salary Positions') plt.show() Output: Gradient Boosting Regression apply gradient boosting technique there difficulties with classification regression methods. predictive model built using numerous smaller prediction models; these tiny models often decision trees. loss function necessary Gradient Boosting Regressor perform. Gradient boosting regressors handle variety predefined loss functions addition supporting customized loss functions; however, loss function differentiable. Even though regression methods generally logarithmic function, squared errors also employed regression techniques. don't have construct loss function each progressive boosting stage gradient boosting algorithms; instead, choose whatever differentiable loss function. Code Python program perform regression using Gradient Boosting Regression algorithm Importing required modules import numpy from sklearn.datasets import load_diabetes from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import cross_val_score Loading diabetes dataset load_diabetes(return_X_y True) Separating training validating datasets X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.3, random_state Creating object Gradient Boosting Regressor class gbr GradientBoostingRegressor(n_estimators 150, learning_rate 1.0, max_depth random_state gbr.fit(X_train, Y_train) Computing accuracy score Gradient Boosting Regression model using cross_val_score method score cross_val_score(gbr, Printing scores print("Accuracy scores: scores) print("The mean accuracy score np.mean(scores)) Output: Accuracy scores: [-1.02298964 0.05276312 -0.74850198 -0.07277615 0.47050685 0.3024374 -1.31209035 -0.26358347 0.43173477 -0.19109809 -0.56778646 -0.61486148 -0.11295867 0.0408493 -0.26464188] mean accuracy score -0.25819978131125926 Regression Handle Linear Dependencies reliable method forecasting numerical variables regression. machine learning techniques above include effective regression methods that sklearn Python library perform regression analysis forecasting various machine learning tasks. regression better option when dataset linear correlations between independent dependent characteristics. Other regression algorithms, like neural networks, employed manage non-linear connections between data features because they record non-linearity using activation functions. Next TopicCOVID-19 Data Representation using Tkinter Python prev next