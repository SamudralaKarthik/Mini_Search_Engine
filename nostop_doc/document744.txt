next prev Apache Airflow Python Airflow Python Operator this tutorial, will learn about Apache Airflow operators. will discuss operators airflow however primary explore Python operators Before dive deep into this topic let's understand basic concept Airflow popular. What Data Pipelines? Data pipelines contain multiple tasks actions that must executed desired result. example want create weather that forecasts weather upcoming week. need perform following tasks implement this live weather dashboard. Fetch weather forecast data from weather API. Make some transformations fetched data (e.g., converting temperatures from Fahrenheit Celsius vice versa) that data suits purpose. Push transformed data weather dashboard. that these tasks pipeline. Moreover, these tasks required perform specific order. What Apache Airflow? Apache Airflow tool that popularly used data engineering fields. workflow engine that easily schedules runs complex data pipeline. assures that each task data pipeline will executed order each task gets required resources. provides excellent user interface monitors fixes issues. Airflow uses (Directed Acyclic Graph), collection tasks users want run. These tasks organized such that relationship dependencies maintained. structure (tasks their dependencies) specified code Python scripts. data pipeline best make task relationships more apparent. node represents task, directed edges represent dependencies between tasks. example, task connected with edge pointed task task must finished before task begin. This direction makes directed graph. Installation Airflow install Airflow, will following command. install apache-airflow Once airflow installed, start initializing metadata base database where Airflow stored) using below command. airflow init Now, start apache airflow scheduler airflow schedular important create dags folder airflow directory where will define DAGS. open browser visit http://localhost:8080/admin/ will look like below. Python Operator Apache Airflow There multiple operators Apache Airflow, such BashOperator, PythonOperator, EmailOperator, MySqlOperator, etc. operator specifies single workflow task, operators provide many operators different tasks. this section, will discuss Python operator. Importing Libraries importing required libraries from datetime import timedelta from airflow import from airflow.operators.python_operator import PythonOperator from airflow.utils.dates import days_ago Defining Argument need pass argument dictionary each DAG. Below description argument that pass. owner -The name owner workflow. should alphanumeric have underscores include spaces. depends_on_past -We need mark past true each time workflow; otherwise, mark False. start_date specifies start_date workflow. email represents email-id that receive email whenever task fails reason. retry_delay represents time when task fails long should wait retry Let's understand following example Example These args will passed python operator default_args 'owner': 'lakshay', 'depends_on_past': False, 'start_date': days_ago(2), 'email': ['airflow@example.com'], 'email_on_failure': False, 'email_on_retry': False, 'retries': 'retry_delay': timedelta(minutes=5), 'queue': 'bash_queue', 'pool': 'backfill', 'priority_weight': 'end_date': datetime(2016, 'wait_for_downstream': False, 'dag': dag, 'sla': timedelta(hours=2), 'execution_timeout': timedelta(seconds=300), 'on_failure_callback': some_function, 'on_success_callback': some_other_function, 'on_retry_callback': another_function, 'sla_miss_callback': yet_another_function, 'trigger_rule': 'all_success' Defining Python Function Now, will define Python function that will print given string using argument, Python operator will later this function. new_func(str1): 	return str1 'is fantastic tool' Defining next step create object pass dag_id. dag_id name must unique. Then pass argument that defined earlier description schedule_interval. will after specified interval time. Let's following example. DAG( 'python_operator_sample', default_args=default_args, description='Learn Python Operator?', schedule_interval=timedelta(days=1), Defining Task workflow, have defined only task print task, will print string value terminal using python function. will pass task_id Python Operator object. will name nodes Graph view DAG. python_callable argument, pass function name that want execute pass parameter value "op_kwargs" dictionary finally, object which want link this task. PythonOperator( task_id='print', python_callable= new_function, op_kwargs {"x" "Apache Airflow"}, dag=dag, Now, refresh Airflow dashboard; will show list. Each step workflow will separate box; click wait until border turns dark green, indicating that completed successfully. Click node "print" more details about this step, then click Logs, will output like this. What Variables Apache Airflow? discussed, airflow used create manage complex workflows. multiple workflows same time. workflow database same file path. Now, change directory path where user saves files changes database configuration. that case, don't want update each DAGS distinctly. Using airflow, create variables where store retrieve data runtime multiple DAGS. want make change, edit variable, workflow good create Variables? create variable, open Airflow click Admin from menu then click Variables. Click Create button create variable window will open. value submit. Now, will create where will find word count text data this file. import newly created variables. Let's understand following example. Example from datetime import timedelta from airflow import from airflow.operators.python_operator import PythonOperator from airflow.utils.dates import days_ago from airflow.models import Variable will define function that will variable, read calculate word count. from collections import Counter define python function my_function(): variable value file_path Variable.get("data_path") open file file_ open(file_path) read file calculate word count data Counter(file_.read().split()) return data Now, steps same above, need define task workflow ready run. DAG( 'word_count_example', default_args=default_args, description='How Python Operator?', schedule_interval=timedelta(days=1), define first task PythonOperator( task_id='print_word_count', python_callable= my_function, dag=dag, When DAG, will show word count. also edit whenever want DAGS updated. Conclusion this tutorial, have discussed Python Operator Apache Airflow normal variables branching. have understood basic concepts Apache Airflow installation. Next TopicCurrying Python prev next