next prev Project Python Breast Cancer Classification with Deep Learning malignant condition known breast cancer brought unchecked growth cells breast. Early treatment diagnosis have been made possible advancements diagnosis prevention. Women should aware following symptoms signs: lump mass, nipple retraction, nipple breast pain, redness, thickening nipple, scaliness breast skin, nipple discharge (other than breast milk) biopsy usually ordered when doctor suspects breast cancer. small amount breast tissue taken under microscope during examination procedure. Doctors classify each breast cancer case according standard parameters like disease's type, stage, grade, gene expression provide most effective treatment. cancer care medical team will determine prognosis treatment plan following confirmation diagnosis. Based Breast Cancer Types Classification When tissue examined during biopsy, determined whether there cancer, adenocarcinoma, another form, such sarcoma. location severity disease typically used classify cancer types. Ductal carcinoma most prevalent, meaning that illness first developed inside ducts (pipes) carrying milk toward nipple. cancer spread outside duct categorized situ (DCIS), invasive therefore contained within duct; otherwise, described metastatic breast cancer. Other subtypes include triple-negative, inflammatory prostate, advanced breast cancer, other uncommon breast cancer types. Based Breast Cancer Grades Classification disease graded pathologists well. grade determined tissue biopsy appears, similar appears healthy cells, quickly cancer cells divide. lower grade typically denotes slower rate growth reduced risk metastasis, whereas higher grade denotes faster rate growth higher risk spreading. Bloom-Richardson degree, Nottinghamshire grade, Device grade, Appeals young grade used describe histologic tumor grade. Grade denotes well differentiation (closely resembling normal cells), while grade denotes moderate differentiation. Poorly differentiating breast cancer classified grade (almost unrecognizable fast-growing.) Breast Cancer Based Genes Expression Classification American Cancer Society claims epigenetics additional relatively method classifying breast cancer. Gene expression classifies breast tumors into four groups based PAM50 test's determination their molecular characteristics. Breast cancer types with luminal hydrophilic gene expression patterns resemble tissue. Low-grade tumors with slow growth Luminal malignancies. Luminal malignancies more aggressive have higher rate growth. When extra copies Target gene exist, HER2 breast cancer identified. basal type, member triple-negative types, identified when there normal levels detectable estrogen progesterone receptors. prevalence higher females have BRCA1 mutation. Basal cancer more prevalent younger African-American women unclear causes. website Breast Cancer News solely focused delivering news details about condition. doesn't offer medical guidance, diagnosis, care. This information does replace qualified medical guidance, diagnosis, care. Make sure consider obtaining competent medical advice from what have found this website. Always your doctor another knowledgeable health provider advice have concerns about medical condition. Learn terminologies used Python Breast Cancer Classification project. Deep Learning: What Deep Learning rigorous machine learning method that draws inspiration from biological networks neurons human brain. Convolutional neural networks, deep neural systems, perceptron, deep belief networks architectures that numerous layers pass data through before creating output. Deep Learning used numerous domains, including computer vision, natural language processing, computational linguistics, audio recognition, medication creation, advance enable many applications. What Keras? Python-based Keras free neural network library. greater used with Theano, CNTK, Python. main goal Keras provide quick experimentation prototyping that works smoothly GPU. expandable, modular, user-friendly. Breast Cancer Classification About Python Project We'll create classifier this Python project train breast cancer histopathological image dataset. We'll save information validation. We'll create (Artificial Neural Network), dubbed Cancernets, using Keras train using image data. After that, we'll create discriminant function evaluate model's effectiveness. IDC, invasive ductal carcinoma, most prevalent type breast cancer, accounting diagnoses. begins milk duct spreads fibrous fatty cervix outside channel. study something like microscopic structure underlying tissues called histology. objective Breast Cancer Classification Constructing Breast Prostate Cancer classifier using collection that correctly identify benign malignant histological image. Dataset We'll Kaggle's regular dataset, which contains breast cancer histology picture data. This dataset contains 2,77,52566 5050 patch images prostate cancer tissues scanned magnifications from 162 entire mount slide images. 1,98,768 them suffer tests, while 78,786 receive positive ones. data openly accessible. 6.02Megabytes disc space required very least this. Prerequisites You'll need install Python packages execute this sophisticated Python project. accomplish this. install numpy opencv-python pillow tensorflow Keras imutils scikit-learn matplotlib Approach Advanced Project Python Breast Cancer Classification Step this file. Please arrive preferred location unzip Step Now, create datasets directory breast cancer classification; within this, create original: Step Original mkdir datasets mkdir datasets Make copy dataset. Step dataset unzipped original directory. will tree command examine this directory's structure: breast cancer classification, original tree, datasets, breast cancer classification Output: ((base) \Users\Sumeet Rathore\Desktop\breast-cancer-classification\breast-cancer-classification\datasets\original?tree Folder PATH listing volume windows volume serial number 02FE-40C9 -10253 -10254 -10255 -10256 -10257 -10258 -10259 -10260 each patient have directory. also have directories images with benign malignant content each these directories. File name: config.py will require some configurations construct dataset train model. This found cancernets directory. import INPSUT_DATASET "datasets/original" BASE__PATH "datasets/idc" TRAIN_PATH os.path.sep.join([BASE_PATH, "training"]) VAL_PATH os.path.sep.join([BASE_PATH, "validation"]) TEST_PATH os.path.sep.join([BASE_PATH, "testing"]) TRAIN_SPLIT VAL_SPLIT Explanation: this section, specify base path paths training, validation, testing directories, path input dataset (datasets/original), directory (datasets/IDC). Additionally, declare that training will consume dataset, validation will consume 10%. File name: build_datasets.py This will divide dataset into training, validation, testing sets according above ratio: testing training. will extract images batches using Keras ImageDataGenerator avoid taking memory space entire dataset once. from cancernets import config from imutils import paths import random, shutil, originalPaths list(paths.list_images(config.INPSUT_DATASET)) random.seed(7) random.shuffle(originalPaths) index int(len(originalPaths)*config.TRAIN_SPLIT) trainpsaths originalPaths[:index] testPaths originalPaths[index:] index int(len(trainpsaths)*config.VAL_SPLIT) valPaths trainpsaths[:index] trainpsaths trainpsaths[index:] datasets [("training", trainpsaths, config.TRAIN_PATH), ("validation", valPaths, config.VAL_PATH), ("testing", testPaths, config.TEST_PATH)] (setType, originalPaths, basePath) datasets: print(f'Building {setType} set') os.path.exists(basePath): print(f'Building directory {base_path}') os.makedirs(basePath) path originalPaths: file path.split(os.path.sep)[-1] label file[-5:-4] labelPath os.path.sep.join([basePath,label]) os.path.exists(labelPath): print(f'Building directory {labelPath}') os.makedirs(labelPath) new__Path os.path.sep.join([labelPath, file]) shutil.copy2(inpsutPath, new__Path Explanation: Imports from shutil, imutils, config, random, will used this. After shuffle, will compile list original paths images. length this list then multiplied calculate index, which enables slice into sublists training testing datasets. remainder list used training itself, with list going training dataset validation. Now, list tuples containing information about training, validation, testing sets called dataset. each, these hold paths well base path. will print, instance, "Building testing set" each setType, path, base path this list. will make directory there base path. Additionally, will extract class label filename each originalPaths path. will construct path label directory does already exist, will explicitly create will construct path final image copy where belongs. Step Execute build_datasets.py script build_datasets.py Output Screenshot: \Users \ADMIN\Desktop\breast-cancer-classification\breast-cancer-classification>pybuild_dataset.py Building training Building directory datasets/idc\training Building directory datasets/idcltraining\e Building directory datasets/ide\training\1 Building validation Building directory datasets/idclvalidation Building directory datasets/idc\validation\1 Building directory datasets/idclvalidationle Building testing Building directory datasets/idcltesting Building directory datasets/idcltesting\1 Building directory datasets/idcltesting(0) File name: cancernets.py Cancernets will name network build: (Convolutional Neural Network). This network carries following operations: Make three CONV filters; Stack these filters another; carry maximum pooling; employ depthwise separable convolution, which more effective uses less memory. Quickly, baseline model overfit. model made more consistent with both dropout placement settings. see, model could achieve higher training accuracy placing dropout layer after pooling layer. TensorFlow employs element-wise dropout, which activation some neurons randomly masked multiplying zero. Anyways, still, same number actual neurons present. Therefore, applying pooling operation this equivalent applying pooling operation output conv layer. other hand, when pooling followed dropout, randomly remove some neurons from already reduced neurons. have fewer neurons which train this situation. Although using model's full potential this situation, regularization appears stronger. model with dropout layer following pooling layer better generalizability. This apparent from test error rate. There right wrong dropout layer convolutional block. other hand, dropout after pooling layer significantly reduces model capacity. conv blocks, batch normalization highly recommended. code section, will figure this. from keras.models import Sequential from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import SeparableConv1D from keras.layers.convolutional import MaxPooling1D from keras.layers.core import Activation from keras.layers.core import Flatten from keras.layers.core import Dropout from keras.layers.core import Dense from keras import backend class Cancernets: @staticmsethod build(width, height, depth, classes): modelss= Sequential() shape (height, width, depth) channelDims K.image_data_format() "channels_first": shape (depth, height, width) channelDims models.add(SeparableConv1D(61, (6,6), paddings "same",inpsut_shape shape)) models.add(Activation("relu")) models.add(BatchNormalization(axis channelDims models.add(MaxPooling1D(pool_size (1,1))) models.add(Dropout(0.15)) models.add(SeparableConv1D(64, (6,6), paddings "same")) models.add(Activation("relu")) models.add(BatchNormalization(axis channelDims models.add(SeparableConv1D(64, (6,6), paddings "same")) models.add(Activation("relu")) models.add(BatchNormalization(axis channelDims models.add(MaxPooling1D(pool_size (1,1))) models.add(Dropout(0.15)) models.add(SeparableConv1D(118, (6,6), paddings "same")) models.add(Activation("relu")) models.add(BatchNormalization(axis channelDims models.add(SeparableConv1D(118, (6,6), paddings "same")) models.add(Activation("relu")) models.add(BatchNormalization(axis channelDims models.add(SeparableConv1D(118, (6,6), paddings "same")) models.add(Activation("relu")) models.add(BatchNormalization(axis channelDims models.add(MaxPooling1D(pool_size (1,1))) models.add(Dropout(0.15)) models.add(Flatten()) models.add(Dense(156)) models.add(Activation("relu")) models.add(BatchNormalization()) models.add(Dropout(0.5)) models.add(Dense(classes)) models.add(Activation("softmax")) return models Explanation: Cancer news constructed using Sequential API, while depthwise convolutions implemented using SeparableConv2D. static method creates Cancernets class four inputs: image's width height, thickness (the number color information each image), well number classes network will forecast between, which initialize models shapes this procedure. modify shape stream dimension when using channels first. Three DEPTHWISE CONV RELU POOL layers will defined, each with higher layering more filters. softmax classifier produces projection percentages each class. Finally, deliver model. previously stated, batch normalization enables train models faster learning rate, allowing network converge more quickly while avoiding internal covariate shifts. This effect demonstrated this experiment. won't believe effective straightforward concept normalizing input hidden layers Observations model with learning rate 0.01 trained without performed better when using Batch Normalization (BN) layers. non-BN model 37,222 trainable parameters, while BN-based model 37,734. result, number trainable parameters increased only 522, those additional trainable parameters cause significant change learning process. point that hidden representation normalization effective. test error rate decreased 29.1 percent with same config layers. This shows that effective. 10-class classification model, non-BN model's test error rate 90%, which random guessing. result, speed training process achieve faster convergence with appropriate learning rate scheduling using batch normalization. Batch normalization, previously discussed, enables train models higher learning rate, allowing network converge more quickly while preventing internal covariate shifts. This experiment demonstrates this outcome. You'll astounded powerful seemingly basic concept normalizing input convolution neurons Observations With Batch Normalization (BN) layers, deep learning algorithm without proportional gain 0.01 outperformed other models. BN-based model 37,834 trainable parameters, compared 37,322 non-BN model. Thus, learning process stayed same more trainable parameters that were included. mandatory item remember that normalizing concealed forms works. File name: train_models.py This evaluates trains model. Let's import from numpy, keras, cancernets, sklearn, config, matplotlib, imutils, import matplotlib matplotlib.use("Agg") from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import LearningRateScheduler from keras.optimizers import Adagrad from keras.utils import nps_utils from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from cancernets.cancernets import Cancernets from cancernets import config from imutils import paths import matplotlib.pyplot plts import numpy import NUM_EPOCHS INIT_LR 1e trainpsaths list(paths.list_images(config.TRAIN_PATH)) lenTrains len(trainpsaths) lenVal len(list(paths.list_images(config.VAL_PATH))) lenTest len(list(paths.list_images(config.TEST_PATH))) trainLabels [int(p.split(os.path.sep)[-1]) trainpsaths] trainLabels nps_utils.to_categorical(trainLabels) classTotals trainLabels.sum(axis classWeight classTotals.max()/classTotals trainAug ImageDataGenerator( rescale 1/155.0, rotation__range zoom__range 0.05, width__shift_range 0.1, height__shift_range 0.1, shear__range 0.05, horizontal__flip True, vertical__flip True, fill__mode "nearest") valAug ImageDataGenerator(rescale 155.0) trainGen trainAug.flow_from_directory( config.TRAIN__PATH, class__mode "categorical", target__size (41,41), color__mode "rgb", shuffle True, batch_size valGen valAug.flow_from_directory( config.VAL_PATH, class_mode "categorical", target_size (41,41), color_mode "rgb", shuffle False, batch_size testGen valAug.flow_from_directory( config.TEST_PATH, class_mode "categorical", target_size (41,41), color_mode "rgb", shuffle False, batch_size modelss= Cancernets.build(width 41,height 41,depth 6,classes otps Adagrad(LR INIT_LR,decay INIT_LR/NUM_EPOCHS) models.compile(loss "binary_crossentropy",optimizer otps,metrics ["accuracy"]) models.fit_generator( trainGen, steps_per_epoch lenTrain//BS, validation_data valGen, validation_steps lenVal//BS, class_weight classWeight, epochs NUM_EPOCHS) print("Now evaluating models") testGen.reset() pred_indices models.predict_generator(testGen,steps (lenTest//BS)+1) pred_indices nps.argmax(pred_indices,axis print(classification_report(testGen.classes, pred_indices, target_names testGen.class_indices.keys())) cms confusion_matrix(testGen.classes,pred_indices) total sum(sum(cms)) accuracy (cms[0,0]+cms[1,1])/total specificitys cms[1,1]/(cms[1,0]+cms[1,1]) sensitivity cms[0,0]/(cms[0,0]+cms[0,1]) print(cms) print(f'Accuracy: {accuracy}') print(f'Specificity: {specificity}') print(f'Sensitivity: {sensitivity}') NUM_EPOCHS plts.style.use("ggplot") plts.figure() plts.plot(nps.arange(0,N), M.history["loss"], label "train__loss") plts.plot(nps.arange(0,N), M.history["val__loss"], label "val__loss") plts.plot(nps.arange(0,N), M.history["acc"], label "train_acc") plts.plot(nps.arange(0,N), M.history["val_acc"], label "val_acc") plts.title("Trainings Loss Accuracy Dataset") plts.xlabel("Epoch Number") plts.ylabel("Loss//Accuracy") plts.legend(loc "lowers left") plts.savefig('plots.png') Explanation: begin this script setting initial values batch size, learning rate, number epochs. number training, validation, testing paths three directories will obtained. class weight training data will then obtained correct imbalance. training data augmentation object will initialized. Regularization process making models more general. slightly modify training examples here avoid requiring additional training data. validation testing data augmentation objects will initialized. Now, call fit.generator() model. project dataset training, validation, testing generators will initialized produce batches images with size batch_size. After that, will Adagrad optimizer initialize models binary_crossentropy loss function compile them. models have been successfully trained. Let's evaluate models using test data. will reset generator data make predictions. indices labels with highest predicted probability images from testing then obtained. Additionally, classification report will shown. that have accuracy, specificity, sensitivity, will compute confusion matrix display values. Last least, we'll plot accuracy training loss. Output console screen: WARNING: tensorflow: From \Users \ADMIN\ AppData \Local \Programs \Python \Python37 \lib\site-packages\tensorflow\python \ops \nn loved future version. imp1.py: 180: add_dispatch_support.?locals>.wrapper (from tensorflow.python.ops.array_ops) deprecated will Instructions updating tf. Wherein 2.0, which same broadcast rule p.where Epoch 1/40 6244/6244 0.8370 25905 415ms/step loss: 0.3717 acc: 0.8422 loss: 0.4139 acc: Epoch 2/40 6244/6244 0.8471 I= 24985 400ms/step loss: 0.3464 acc: 0.8527 loss: 0.3955 Epoch 3/40 6244/6244 24805 397ms/step loss: 0.3416 acc: 0.8552 loss: 0.4203 acc: 0.8423 Epoch 4/40 6244/6244 24985 400ms/step loss: 0.3396 aCC: 0.8562 loss: 0.4028 acc: 0.8414 Epoch 5/40 6244/ 6244 24395 391ms/step loss: 0.3388 acc: 0.8573 loss: 0.3880 acc: 0.8461 Epoch 6/40 6244/6244 ==]- 24205 388ms/step loss: 0.3366 acc: 0.8573 loss: 0.3868 acc: 0.8460 Epoch 7/40 6244/6244===] 24085 386ms/step loss: 0.3363 acc: 0.8578 loss: 0.3879 acc: 0.8456 Epoch 8/40 6244/6244= 24175 387ms/step loss: 0.3355 acc: 0.8580 loss: 0.3854 acc: 565 6244/6244 0.8529 Epoch 38/40 6244/6244 0.8513 Epoch 39/40 6244/ 6244 0.8511 Epoch 40/40 6244/6244 0.8516 evaluating model precision. OX 2382s 381ms/step loss: 0.3303 acc: 0.8605 loss: 0.3807 acc: 2384s 382ms/step loss: 0.3308 acc: 0.8599 loss: 0.3854 acc: 2676s 429ms/step loss: 0.3317 acc: 0.8594 loss: 0.3847 acc: 23795 381ms/step loss: 0.3318 acc: 0.8598 loss: 0.3848 acc: recall f1-score support- 0.91 0.72 0.87 0.79 0.89 0.75 39736 15769 accuracy macro weighted 0.71 0.86 0.83 0.85 0.85 0.82 0.85 55505 557505 55505 ?Data [[34757 4979] 3271 12498]] Accuracy: 0.8513647419151428 Specificity: 0.79256769617579512626 Sensitivity: 0.8746980068451782 \Users \ADMIN\Desktop\breast-cancers-classifications\breast-cancer-classification> expected output graph looks like this: Summary this Python project, developed network Cancernets dataset learned build breast prostate cancer classifier using dataset (featuring histology images Advanced Ductal Carcinoma). same, made advantage keras library. hope this Python project enjoyable. Next TopicColour game using PyQt5 Python prev next