next prev Kafka Tutorial Python following tutorial, will discuss Apache Kafka along with Python programming language. Understanding Apache Kafka Apache Kafka open-source stream platform that originally designed LinkedIn. Later, handed over Apache Foundation open-sourced 2011. definition from Wikipedia: Apache Kafka open-source platform developed Apache Software Foundation used processing streams. written Java Scala. goal project offer high-throughput, unified, low-latency platform order handle real-time data feeds. storage layer Apache Kafka fundamentally "massively scalable pub/sub message queue designed distributed transaction log," that makes extremely valuable enterprise infrastructures order process streaming data. Moreover, Kafka connects external systems (for importing exporting data) through Kafka Connect offers Kafka Streams, library Java stream processing. think giant commit where store data order happening. users this just access utilize their needs. Some Cases Apache Kafka Apache Kafka different places. consider some cases Kafka that could help figure usage: Activity Monitoring:We Kafka monitor activities. activity could belong physical sensor device website. Producers publish data from data sources that later utilized find trends patterns. Messaging:We also Kafka message broker among services. implementing microservice architecture, have microservice producer another consumer. example, have microservice responsible creating accounts sending emails users related account creation. Aggregation:We also utilize Kafka collecting logs from distinct systems store them centralized system further processing. ETL: Kafka offers feature almost real-time streaming; hence, develop based requirement. Database:Based things have mentioned earlier, that Kafka also acts database. typical database that feature data querying requirement, Kafka store data long require without consuming Understanding Concepts Kafka discuss core concepts Kafka. Topics:Every message that feed into system must part some topic. topic stream records. messages store format key-value pairs. Every message assigned sequence, known Offset. result message could input other further processing. Producers:Producers applications responsible publishing data into Kafka system. They publish data topic their choice. Consumer: There Consumers applications that uses messages published into topics. consumer gets subscription topic preference consumes data. Broker: broker instance Kafka which responsible message exchange. Kafka part cluster stand-alone machine. Now, consider simple example, there go-down warehouse restaurant where material stored, such vegetables, rice, flour, more. restaurant serves various kinds dishes like Indian, Italian, Chinese many more. cooks each cuisine refer warehouse select required objects make dishes. There chance that cooks from different cuisine same stuff made materials. This secret ingredient that utilized every kind dish. following case, warehouse acts broker, merchants goods producers, goods secret ingredient created cooks topics, cooks consumers. access Kafka Python? There various libraries available Python programming language Kafka. Some these libraries described below: Library Description Kafka-Python This open-source library designed Python community. PyKafka This library maintained Parsly claimed Pythonic API. However, cannot create dynamic topics this library like Kafka-Python. Confluent Python Kafka This library provided Confluent thin wrapper around librdkafka. Thus, performs better than above two. Installing Dependencies will Kafka-Python this project. install manually using installer shown below: Syntax: install kafka-python Now, start building project. Project Code following example, will create producer that produces numbers ranging from send them Kafka broker. Later consumer will read that data from broker keep them MongoDB collection. benefits utilizing Kafka that case consumer breaks down, another fixed consumer will continue reading where earlier left. This good method confirm that data into database without missing data duplicates. following example, create Python program file named produce.py begin with importing some required libraries modules. File: produce.py importing required libraries from time import sleep from json import dumps from kafka import KafkaProducer Explanation: above snippet code, have imported required libraries modules. Now, initialize Kafka producer. Note following parameters: bootstrap_servers ['localhost: 9092']: This parameter sets host port contact producer bootstrap initial cluster metadata. mandatory this here, default host port localhost: 9092. value_serializer lambda dumps(x).encode('utf-8'): This parameter functions serialization data before sending broker. Here, transform data into JSON file encode UTF-8. consider following snippet code same. File: produce.py initializing Kafka producer my_producer KafkaProducer( bootstrap_servers ['localhost:9092'], value_serializer lambda x:dumps(x).encode('utf-8') Explanation: above snippet code, have initialized Kafka producer using KafkaProducer() function, where have used parameters described above. Now, have generate numbers ranging from 500. perform this using for-loop where every number value dictionary with key: num. This used data only, topic. Within same loop, will also send data broker. perform this calling send method producer detailing topic data. Note: value serializer will automatically transform encode data. take five seconds break order conclude iteration. case have confirm whether broker received message, advised include callback. File: produce.py generating numbers ranging from range(500): my_data {'num' n} my_producer.send('testnum', value my_data) sleep(5) Explanation: above snippet code, have used for-loop iterate number ranging from 500. have also added interval five seconds between each iteration. somebody wants test code, recommended create topic send data that newly generated topic. This method will avoid case duplicate values possible confusion testnum topic when will testing producer consumer together. Consuming Data Before started with coding part consumer, create Python program file name consume.py. will import some modules such json.loads, MongoClient KafkaConsumer. Since PyMongo scope this tutorial, won't digging deeper into code. Moreover, somebody also replace mongo code with other code needs. code this order enter data into another database, code process data, anything else think consider following snippet code, begin with. File: consume.py importing required modules from json import loads from kafka import KafkaConsumer from pymongo import MongoClient Explanation: above snippet code, have imported required modules from their respective libraries. create Kafka Consumer. will KafkaConsumer() function this work; let's have closer look parameters used this function. Topic: first parameter KafkaConsumer() function topic. following case, testnum. bootstrap_servers ['localhost: 9092']: This parameter same producer. auto_offset_reset 'earliest': This parameter among other significant parameters. handles where consumer restarts reading after being turned breaking down either latest earliest. Whenever earliest, consumer begins reading latest committed offset. Whenever latest, consumer begins reading log's end. that exactly what need here. enable_auto_commit True: This parameter confirms whether consumer commits read offset each interval. auto_commit_interval_ms 1000ms: This parameter used interval between commits. messages coming every interval five seconds, committing every second appears fair. group_id 'counters': This parameter group consumers which consumer belongs. Note that consumer must part consumer group order make them work automatically committed. value deserializer used deserialize data into general JSON format, inverse working value serializer. consider following snippet code same. File: consume.py generating Kafka Consumer my_consumer KafkaConsumer( 'testnum', bootstrap_servers ['localhost 9092'], auto_offset_reset 'earliest', enable_auto_commit True, group_id 'my-group', value_deserializer lambda loads(x.decode('utf-8')) Explanation: above snippet code, have used KafkaConsumer() function generate Kafka Consumer. have also added parameters within function that studied earlier. Now, consider following snippet code connect testnum collection (This collection similar table relational database) MongoDB database. File: consume.py my_client MongoClient('localhost 27017') my_collection my_client.testnum.testnum Explanation: above snippet code, have defined variable my_client that uses MongoClient() function specified with host port. have then defined another variable my_collection that uses my_client variable access data testnum topic. This data extracted from consumer looping through (here, consumer considered iterable). consumer will keep listening until broker does respond anymore. access message value using value attribute. Here, overwrite message with message value. next line inserts data into database collection. last line will print confirmation that message added collection. Note: possible insert callbacks actions this loop. File: consume.py message my_consumer: message message.value collection.insert_one(message) print(message added my_collection) Explanation: above snippet code, have used for-loop iterate through consumer order extract data. order test code, execute produce.py file first then consume.py. Next TopicAugmented Assignment Expressions Python prev next