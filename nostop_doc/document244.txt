next prev Python Data Analytics Data Analysis help obtain useful information from data provide solution queries. Further, based observed patterns predict outcomes different business policies. Understanding basic Data Analytics Data kind data which work during analysis mostly (comma separated values) format. Usually, first files represents headers. Packages Available There's diversity libraries available Python packages that facilitate easy implementation without writing long code. Examples some packages are- Scientific computing libraries such NumPy, Pandas SciPy. Visualization libraries such Matplotlib seaborn. Algorithm libraries such scikit-learn statsmodels. Importing Exporting Datasets essential things that must take care while importing datasets are- Format- refers file encoded. examples prominent formats .csv,.xlsx,.json, etc. Path File- path file refers location file where stored. available either drives some online source. done following way- Example import pandas path=" pd.read_csv(path) dataset doesn't contain header, specify following way- pd.read_csv(path,header=None) look first five last five rows dataset, make df.head() df.tail() respectively. Let's have look export data, have file present .csv format then, path df.to_excel(path) Data Wrangling Data wrangling process converting data from format which used analysis what this part encompasses- deal with missing values? Missing values Some entries left blank because unavailability information. usually represented with NaN, discuss deal with them- best option replace numerical variable with their average categorical variable with mode. Sometimes situation might occur, when have drop missing value, done using- df.dropna() want drop row, have specify axis want drop column, have specify axis Moreover, want these changes directly occur dataset, will specify more parameter inplace True. let's values replaced- syntax df.replace(missing value, value) Here will make variable store mean attribute (whose value want replace) mean=df["attribute name"].mean() df["attribute name"].replace(np.nan,mean) proceed with data formatting? refers process bringing data comprehensible format. example Changing variable name make understandable. Normalization Data features present dataset have values that result biased prediction. Therefore, must bring them range where they comparable. same, following techniques attribute- Simple Feature Scaling Xn=Xold/Xmax Min-Max approach Xn=Xold-Xmin/Xmax-Xmin Z-score Xn=Xold-µ/Ꝺ µ average value Ꝺ-standard deviation convert categorical variables into numeric variables? Under this, proceed with process called "One-Hot Encoding", let's there's attribute that holds categorical values. will make dummy variables from possibilities assign them based their occurrence attribute. convert categorical variables dummy variables will pandas.get_dummies(df["attribute-name"]) This will generate expected results. Binning Python refers process converting numeric variables into categorical variables. Let's have taken attribute 'price' from dataset. divide data into three categories based range then denote them with names such low-price, mid-price, high price. obtain range using linspace() method bin np.linspace(min(df["attribute-name"]),max(df["attribute-name"]),4) cat_names=["low-price","mid-price","high-price"] df["bin_name"]=pd.cut(df["attribute-name"],bin,labels=cat_names) Exploratory Data Analysis Statistics find statistical summary dataset using describe() method. used df.describe(). categorical variables summarized using value_counts() method. Using GroupBy groupby() method pandas applied categorical variables. groups subsets based different categories. involve single multiple variables. have look example that would help understand used Python. df_att=df[['attribute1', 'attribute2', 'attribute3']] df_g=df_att.groupby(['attribute1', 'attribute2'], as_index=False).mean() df_g Correlation Correlation measures scope which variables interdependent. visual idea checking what kind correlation exists between variables. plot graph interpret does rise value attribute affects other attribute. Concerning statistics, obtain correlation using Pearson Correlation. gives correlation coefficient P-value. have look criteria- CORRELATION COEFFICIENT RELATIONSHIP Close Large Positive Close Large Negative Close relationship exists P-VALUE CERTAINITY P-value<0.001 Strong P-value<0.05 Moderate P-value<0.1 Weak P-value>0.1 piece code using scipy stat package. Let's want calculate correlation between attributes, attribute1 attribute2- pearson_coef,p_value=stats.pearsonr(df["attribute1"],df["attribute2"]). Further check correlation between variables, create heatmap. Relationship between categorical variables relationship between categorical variables calculated using chi-square method. scipy.stats.chi2_contingency(cont_table, correction=True) Develop Model? First, understand what model? model refer equation that helps predict outcomes. Linear Regression Multiple Linear Regression Linear Regression name suggests, involves only single independent variable make prediction. Multiple Regression involves multiple independent variables make prediction. equation simple linear regression represented as- y=b0x+b1 Here, y-dependent variable independent variable b0-slope b1-intercept implement Linear Regression Python- from sklearn.linear_model import LinearRegression lm=LinearRegression() X=df["attribute-name1"] Y=df["attribute-name1"] lm.fit(X,Y) yp=lm.predict(X) Using Visualization evaluate model Creating plots good practice since they show strength correlation whether direction relationship positive negative. have look different plots that help evaluate model- Using Regression Plot import seaborn sns.regplot(x="attribute1",y="attribute2",data=df) plt.ylim(0,) Using Residual Plot import seaborn sns.residplot(df["attribute1"],df["attribute2"]) Sample Evaluation Here, will discuss evaluate model numerically, ways doing same are- Mean Square Error(MSE) This method takes difference between actual predicted value, squares then finally calculates their average. implement same Python using- from sklearn.metrics import mean_squared_error mean_squared_error(df['target-variable'],Y_predict_simple_fit) R-squared R-squared also known coefficient determination. shows closeness data with fitted regression line. used Python using score() method. X=df["attribute-1"] Y=df["attribute-2"] lm.fit(X,Y) lm.score(X,Y) Decision Making nutshell, have take care following things when evaluating model- visualization numerical evaluation methods. Evaluate Model? Evaluating model integral element since tells perfectly data fits model. Now, will discuss training data predict results. idea split dataset into training testing. training dataset used build model testing dataset used assess performance model. implemented Python using- from sklearn.model_selection import train_test_split x_train,y_train,x_test,y_test=train_test_split(x_data,y_data,test_size=' ',random_state=' Overfitting Underfitting Overfitting- condition when model quite simple data. Underfitting condition when model easily adjusts noise factor rather than function. Ridge Regression This used when dealing with variables tenth degree. Here introduced factor called alpha. implement this Python. from sklearn.linear_model import Ridge RModel=Ridge(alpha=0.1) RModel.fit(X,Y) Yp=RModel.predict(X) Next TopicPython seek() Method prev next