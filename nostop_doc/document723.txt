next prev Plot Multiple Linear Regression Python strategy modeling relationship between dependent feature (the target variable) single independent feature (simple regression) multiple independent features (multiple regression) called linear regression. linear regression algorithm works assumption that both types variables have linear relationship. this relationship exists, calculate model's necessary coefficients make forecasts based unseen data. Descriptive Analysis Variables It's usually good idea plot visualization charts data comprehend better check there relationships between each feature before stepping further into applying multi-linear regression. We'll utilize pairplot() method from Seaborn package plot relationships between features. function will produce figure with histogram scatter plot connecting each feature. Using variety libraries, including Numpy, Pandas, Scikit-Learn, Scipy, will learn apply visualize linear regression process Python from scratch this tutorial. Importing Libraries will import some Python libraries need, such NumPy, Pandas, sklearn, matplotlib, etc., first step. Additionally, we'll Pandas library load dataset from GitHub repositories convert dataset into dataframe called df. Code Importing required methods modules Basic libraries import pandas import numpy import warnings building model from sklearn import linear_model data visualization import seaborn import matplotlib.pyplot from mpl_toolkits.mplot3d import Axes3D %matplotlib inline Output: (545, price area bedrooms stories 13300000 7420 12250000 8960 12250000 9960 12215000 7500 11410000 7420 Feature Selection will relationships between features using pair plot. Code Visualizing relationships between features using pair plots sns.pairplot(data housing, height Output: first figure that there linear relationship between price area features dataset. that scatter plots rest variables random show relationship among them. should take only multiple independent features having relationship between them. While here, prices target variable, there need drop feature. Multicollinearity multiple linear regression model assumes correlation exists between predictors independent variables employed regression. Using corr() method from Pandas dataframe, compute Pearson correlation coefficient value between every features data build matrix whether there correlation between predictors. After that, Seaborn's heatmap() plot display matrix heatmap. Code Visualizing multicollinearity between independent features using heatmap corr housing[['area', 'bedrooms', 'stories']].corr() print('Pearson correlation coefficient matrix each independent variable: \n', corr) Masking diagonal cells masking np.zeros_like(corr, dtype np.bool) np.fill_diagonal(masking, True) Initializing matplotlib figure figure, axis plt.subplots(figsize Generating custom colormap c_map sns.diverging_palette(223, as_cmap True, 100) c_map.set_bad('grey') Displaying heatmap with masking correct aspect ratio sns.heatmap(corr, mask masking, cmap c_map, vmin vmax center linewidths figure.suptitle('Heatmap visualizing Pearson Correlation Coefficient Matrix', fontsize axis.tick_params(axis 'both', which 'major', labelsize Output: Pearson correlation coefficient matrix each independent variable: area bedrooms stories area 1.000000 0.151858 0.083996 bedrooms 0.151858 1.000000 0.408564 stories 0.083996 0.408564 1.000000 Building Multiple Linear Regression Model Let's move forward toward developing regression model. that have already seen relationship collinearity between features, features build model. We'll LinearRegression() class Sklearn's linear_model library create models. Code Building Multiple Linear Regression Model Setting independent dependent features housing.iloc[:, 1:].values housing.iloc[:, 0].values Initializing model class from sklearn package fitting data into linear_model.LinearRegression() reg.fit(X, Printing intercept coefficients regression equation print('Intercept: reg.intercept_) print('Coefficients array: reg.coef_) Output: Intercept: 157155.2578429943 Coefficients array: [4.17726303e+02 4.18703502e+05 6.73797188e+05] will attempt convert model into three-dimensional graph using code cell below. data point will shown chart grey dots, linear model will represented blue plane. Code Plotting plot visualizing Multiple Linear Regression Model Preparing data independent housing[['area', 'bedrooms']].values.reshape(-1,2) dependent housing['price'] Creating variable each dimension independent[:, independent[:, dependent x_range np.linspace(5, y_range np.linspace(3, x1_range np.linspace(3, x_range, y_range, x1_range np.meshgrid(x_range, y_range, x1_range) viz np.array([x_range.flatten(), y_range.flatten(), x1_range.flatten()]).T Predicting price values using linear regression model built above predictions reg.predict(viz) Evaluating model using square model reg.score(X, Ploting model visualization plt.style.use('fivethirtyeight') Initializing matplotlib figure plt.figure(figsize (15, axis1 fig.add_subplot(131, projection '3d') axis2 fig.add_subplot(132, projection '3d') axis3 fig.add_subplot(133, projection '3d') axes [axis1, axis2, axis3] axes: ax.plot(x, color='k', zorder linestyle 'none', marker 'o', alpha 0.1) ax.scatter(x_range.flatten(), y_range.flatten(), predictions, facecolor (0,0,0,0), edgecolor '#70b3f0') ax.set_xlabel('Area', fontsize labelpad ax.set_ylabel('Bedrooms', fontsize labelpad ax.set_zlabel('Prices', fontsize labelpad ax.locator_params(nbins axis 'x') ax.locator_params(nbins axis 'x') axis1.view_init(elev=25, azim=-60) axis2.view_init(elev=15, azim=15) axis3.view_init(elev=25, azim=60) fig.suptitle(f'Multi-Linear Regression Model Visualization (R2 {r2})', fontsize color 'k') Output: Next TopicPhysics Calculations Python: Introduction Python Function prev next