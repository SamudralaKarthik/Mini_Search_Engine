next prev Tokenizer Python know, there incredibly huge amount text data available internet. But, most familiar with methods order start working with this text data. Moreover, also know that tricky part navigate language's letters Machine Learning Machines recognize numbers, letters. text data manipulation cleaning done create model? order answer this question, explore some wonderful concepts beneath Natural Language Processing (NLP). Solving problem process divided into multiple stages. First all, have clean unstructured text data before moving modeling stage. There some steps included data cleaning. These steps follows: Word Tokenization Parts Speech prediction every token Text Lemmatization Stop Words Identification Removal, more. following tutorial, will learning more about very primary step known Tokenization. will understanding what Tokenization necessary Natural Language Processing (NLP). Moreover, will also discovering some unique methods execute Tokenization Python. Understanding Tokenization Tokenization said dividing large quantity text into smaller fragments known Tokens. These fragments Tokens pretty useful find patterns deliberated foundation step stemming lemmatization. Tokenization also supports substitution sensitive data elements with non-sensitive ones. Natural Language Processing (NLP) utilized create applications like Text Classification, Sentimental Analysis, Intelligent Chatbot, Language Translation, many more. Thus, becomes important understand text pattern achieve purpose stated above. now, consider stemming lemmatization primary steps cleaning text data with help Natural Language Processing (NLP). Tasks like Text Classification Spam Filtering along with deep learning libraries like Keras Tensorflow. Understanding Significance Tokenization order understand significance Tokenization, consider English Language example. pick sentence keep mind while understanding following section. Before processing Natural Language, have identify words constituting string characters. Thus, Tokenization appears most fundament step proceed with Natural Language Processing (NLP) This step necessary text's actual meaning could interpreted analyzing each word present within text. Now, consider following string example: name Jamie Clark. After performing Tokenization above string, would getting output shown below: ['My', 'name', 'is', 'Jamie', 'Clark'] There various uses performing operation. utilize tokenized form order Count total number words text. Count word's frequency, i.e., total number times specific word present more. Now, understand several ways perform Tokenization Natural Language Processing (NLP) Python. Some Methods perform Tokenization Python There various unique methods performing Tokenization Textual Data. Some these unique ways described below: Tokenization using split() function Python split() function basic methods available order split strings. This function returns list strings after splitting provided string particular separator. split() function breaks string each space default. However, specify separator need. consider following examples: Example 1.1: Word Tokenization using split() function my_text """Let's play game, Would Rather! It's simple, have pick other. Let's started. Would rather Vanilla Ice Cream Chocolate one? Would rather bird bat? Would rather explore space ocean? Would rather live Mars Moon? Would rather have many good friends very best friend? Isn't easy though? When have less choices, it's easier decide. what options would complicated? guess, pretty much understand point, neither first place that Decision.""" print(my_text.split()) Output: ['Let's', 'play', 'a', 'game,', 'Would', 'You', 'Rather!', 'It's', 'simple,', 'you', 'have', 'to', 'pick', 'one', 'or', 'the', 'other.', 'Let's', 'get', 'started.', 'Would', 'you', 'rather', 'try', 'Vanilla', 'Ice', 'Cream', 'or', 'Chocolate', 'one?', 'Would', 'you', 'rather', 'be', 'a', 'bird', 'or', 'a', 'bat?', 'Would', 'you', 'rather', 'explore', 'space', 'or', 'the', 'ocean?', 'Would', 'you', 'rather', 'live', 'on', 'Mars', 'or', 'on', 'the', 'Moon?', 'Would', 'you', 'rather', 'have', 'many', 'good', 'friends', 'or', 'one', 'very', 'best', 'friend?', 'Isn't', 'it', 'easy', 'though?', 'When', 'we', 'have', 'less', 'choices,', 'it's', 'easier', 'to', 'decide.', 'But', 'what', 'if', 'the', 'options', 'would', 'be', 'complicated?', 'I', 'guess,', 'you', 'pretty', 'much', 'not', 'understand', 'my', 'point,', 'neither', 'did', 'I,', 'at', 'first', 'place', 'and', 'that', 'led', 'me', 'to', 'a', 'Bad', 'Decision.'] Explanation: above example, have used split() method order break paragraph into smaller fragments words. Similarly, also break paragraph into sentences specifying separator parameter split() function. know, sentence generally ends with full stop "."; which means that utilize separator split string. consider same following example: Example 1.2: Sentence Tokenization using split() function my_text """Dreams. Desires. Reality. There fine line between dream become desire desire become reality expectations then reality. Nevertheless, live world mirrors, where always want reflect best dream, dream wonder what; dream that want accomplished matter much efforts needed try.""" print(my_text.split('. Output: ['Dreams', 'Desires', 'Reality', 'There fine line between dream become desire desire become reality expectations then reality', 'Nevertheless, live world mirrors, where always want reflect best us', dream, dream wonder what; dream that want accomplished matter much efforts needed try.'] Explanation: above example, have used split() function with full stop parameter order break paragraph full stops. major disadvantage utilizing split() function that function takes parameter time. Hence, only separator order split string. Moreover, split() function does consider punctuations separate fragment. Tokenization using RegEx (Regular Expressions) Python Before moving onto next method, understand regular expression brief. Regular Expression, also known RegEx, special sequence characters that allows users find match other strings string sets with that sequence's help pattern. order start working with RegEx (Regular Expression), Python provides library known library pre-installed libraries Python. consider following examples based word tokenization sentence tokenization using RegEx method Python. Example 2.1: Word Tokenization using RegEx method Python import my_text """Joseph Arthur young businessman. shareholders Ryan Cloud's Start-Up with James Foster George Wilson. Start-Up took flight mid-90s became biggest firms United States America. business expanded major sectors livelihood, starting from Personal Care Transportation 2000. Joseph used good friend Ryan.""" my_tokens re.findall Output: ['Joseph', 'Arthur', 'was', 'a', 'young', 'businessman', 'He', 'was', 'one', 'of', 'the', 'shareholders', 'at', 'Ryan', 'Cloud', 's', 'Start', 'Up', 'with', 'James', 'Foster', 'and', 'George', 'Wilson', 'The', 'Start', 'Up', 'took', 'its', 'flight', 'in', 'the', 'mid', '90s', 'and', 'became', 'one', 'of', 'the', 'biggest', 'firms', 'in', 'the', 'United', 'States', 'of', 'America', 'The', 'business', 'was', 'expanded', 'in', 'all', 'major', 'sectors', 'of', 'livelihood', 'starting', 'from', 'Personal', 'Care', 'to', 'Transportation', 'by', 'the', 'end', 'of', '2000', 'Joseph', 'was', 'used', 'to', 'be', 'a', 'good', 'friend', 'of', 'Ryan'] Explanation: above example, have imported library order functions. have then used findall() function library. This function helps users find words that match pattern present parameter stores them list. Moreover, "\w" used represent word character, refers alphanumeric (includes alphabets, numbers), underscore (_). indicates frequency. Thus, have followed [\w']+ pattern that program should look find alphanumeric characters until encounters other one. Now, let's have look sentence tokenization with RegEx method. Example 2.2: Sentence Tokenization using RegEx method Python import my_text """The Advertisement telecasted nationwide, product sold around states America. product became successful among people that production increased. plant sites were finalized, construction started. Now, Cloud Enterprise became America's biggest firms mass producer major sectors, from transportation personal care. Director Cloud Enterprise, Ryan Cloud, started getting interviewed over success stories. Many popular magazines were started publishing Critiques about him.""" my_sentences re.compile('[.!?] ').split(my_text) print(my_sentences) Output: ['The Advertisement telecasted nationwide, product sold around states America', 'The product became successful among people that production increased', 'Two plant sites were finalized, construction started', "Now, Cloud Enterprise became America's biggest firms mass producer major sectors, from transportation personal care", 'Director Cloud Enterprise, Ryan Cloud, started getting interviewed over success stories', 'Many popular magazines were started publishing Critiques about him.'] Explanation: above example, have used compile() function library with parameter '[.?!]' used split() method separator string from specified separator. result, program splits sentences soon encounters these characters. Tokenization using Natural Language ToolKit Python Natural Language ToolKit, also known NLTK, library written Python. NLTK library generally used symbolic statistical Natural Language Processing works well with textual data. Natural Language ToolKit (NLTK) Third-party Library that installed using following syntax command shell terminal: install --user nltk order verify installation, import nltk library program execute shown below: import nltk program does raise error, then library been installed successfully. Otherwise, recommended follow above installation procedure again read official documentation more details. Natural Language ToolKit (NLTK) module named tokenize(). This module further categorized into sub-categories: Word Tokenize Sentence Tokenize Word Tokenize: word_tokenize() method used split string into tokens words. Sentence Tokenize: sent_tokenize() method used split string paragraph into sentences. consider some example based these methods: Example 3.1: Word Tokenization using NLTK library Python from nltk.tokenize import word_tokenize my_text """The Advertisement telecasted nationwide, product sold around states America. product became successful among people that production increased. plant sites were finalized, construction started. Now, Cloud Enterprise became America's biggest firms mass producer major sectors, from transportation personal care. Director Cloud Enterprise, Ryan Cloud, started getting interviewed over success stories. Many popular magazines were started publishing Critiques about him.""" print(word_tokenize(my_text)) Output: ['The', 'Advertisement', 'was', 'telecasted', 'nationwide', ',', 'and', 'the', 'product', 'was', 'sold', 'in', 'around', '30', 'states', 'of', 'America', '.', 'The', 'product', 'became', 'so', 'successful', 'among', 'the', 'people', 'that', 'the', 'production', 'was', 'increased', '.', 'Two', 'new', 'plant', 'sites', 'were', 'finalized', ',', 'and', 'the', 'construction', 'was', 'started', '.', 'Now', ',', 'The', 'Cloud', 'Enterprise', 'became', 'one', 'of', 'America', "'s", 'biggest', 'firms', 'and', 'the', 'mass', 'producer', 'in', 'all', 'major', 'sectors', ',', 'from', 'transportation', 'to', 'personal', 'care', '.', 'Director', 'of', 'The', 'Cloud', 'Enterprise', ',', 'Ryan', 'Cloud', ',', 'was', 'now', 'started', 'getting', 'interviewed', 'over', 'his', 'success', 'stories', '.', 'Many', 'popular', 'magazines', 'were', 'started', 'publishing', 'Critiques', 'about', 'him', '.'] Explanation: above program, have imported word_tokenize() method from tokenize module NLTK library. Thus, result, method broken string into different tokens stored list. last, have printed list. Moreover, this method includes full stops other punctuation marks separate token. Example 3.1: Sentence Tokenization using NLTK library Python from nltk.tokenize import sent_tokenize my_text """The Advertisement telecasted nationwide, product sold around states America. product became successful among people that production increased. plant sites were finalized, construction started. Now, Cloud Enterprise became America's biggest firms mass producer major sectors, from transportation personal care. Director Cloud Enterprise, Ryan Cloud, started getting interviewed over success stories. Many popular magazines were started publishing Critiques about him.""" print(sent_tokenize(my_text)) Output: ['The Advertisement telecasted nationwide, product sold around states America.', 'The product became successful among people that production increased.', 'Two plant sites were finalized, construction started.', "Now, Cloud Enterprise became America's biggest firms mass producer major sectors, from transportation personal care.", 'Director Cloud Enterprise, Ryan Cloud, started getting interviewed over success stories.', 'Many popular magazines were started publishing Critiques about him.'] Explanation: above program, have imported sent_tokenize() method from tokenize module NLTK library. Thus, result, method broken paragraph into different sentences stored list. last, have printed list. Conclusion above tutorial, have discovered concepts Tokenization role overall Natural Language Processing (NLP) pipeline. have also discussed methods Tokenization (including word tokenization sentence tokenization) from specific text string Python. Next TopicHow lists Python prev next