next prev Dask Python (Part previous tutorial, have understood concept Distributed Computing Introduction Dask. have also understood what Dask Cluster install Dask addition introduction Dask Interface. Dask Interface have already discussed, Dask Interfaces have variety parallel algorithm distributed computation. essential user interfaces being used Data Science Practitioners scale NumPy, Pandas, scikit-learn: Arrays: Parallel NumPy Dataframes: Parallel Pandas Machine Learning: Parallel Scikit-Learn have already covered Dask Array previous tutorial; head straight into Dask DataFrames. Dask DataFrames have observed that requires grouping multiple NumPy Arrays order form Dask Array. Similarly, Dask DataFrame contains numerous smaller Pandas DataFrames. large DataFrame Pandas separates row-wise order form multiple smaller DataFrames. These Smaller DataFrames available Single System Multiple Systems (Hence, allowing store Datasets that larger compared with memory). Every computation Dask DataFrames parallelizes functions prevailing Pandas DataFrames. Here image shown below representing Dask DataFrame Structure: Dask DataFrames also provides Application Programming Interfaces (APIs) that pretty identical Pandas DataFrames offers. Now, consider some examples performing essential functions with Dask DataFrames. Example Reading file Reading file with help Pandas reading file using pandas import pandas my_pdfile pd.read_csv("covid_19_india.csv") print(my_pdfile) Output: Date Time State/UnionTerritory ConfirmedIndianNational ConfirmedForeignNational Cured Deaths Confirmed 30/01/20 6:00 Kerala 31/01/20 6:00 Kerala 01/02/20 6:00 Kerala 02/02/20 6:00 Kerala 03/02/20 6:00 Kerala 9286 9287 09/12/20 8:00 Telengana 266120 1480 275261 9287 9288 09/12/20 8:00 Tripura 32169 32945 9288 9289 09/12/20 8:00 Uttarakhand 72435 1307 79141 9289 9290 09/12/20 8:00 Uttar Pradesh 528832 7967 558173 9290 9291 09/12/20 8:00 West Bengal 475425 8820 507995 [9291 rows columns] Reading file with help Pandas reading file using dask import dask.dataframe my_ddfile ddf.read_csv("covid_19_india.csv") print(my_ddfile.compute()) Output: Date Time State/UnionTerritory ConfirmedIndianNational ConfirmedForeignNational Cured Deaths Confirmed 30/01/20 6:00 Kerala 31/01/20 6:00 Kerala 01/02/20 6:00 Kerala 02/02/20 6:00 Kerala 03/02/20 6:00 Kerala 9286 9287 09/12/20 8:00 Telengana 266120 1480 275261 9287 9288 09/12/20 8:00 Tripura 32169 32945 9288 9289 09/12/20 8:00 Uttarakhand 72435 1307 79141 9289 9290 09/12/20 8:00 Uttar Pradesh 528832 7967 558173 9290 9291 09/12/20 8:00 West Bengal 475425 8820 507995 [9291 rows columns] Explanation: above example, have created different programs. first program, have imported pandas library read_csv() function read file. contrast, have imported dataframe module dask library read_csv() function read file. result both programs will same differ processing time. Dask DataFrames deliver faster speed execute function when compared with Pandas. same noticeable once practically used. Example Finding value count specific column import dask.dataframe my_ddfile ddf.read_csv("covid_19_india.csv") print(my_ddfile.State.value_counts().compute()) Output: Kerala Delhi 283 Rajasthan 282 Haryana Uttar Pradesh Tamil Nadu Ladakh Jammu Kashmir Karnataka Punjab Maharashtra Andhra Pradesh Uttarakhand Odisha 269 West Bengal Puducherry Chhattisgarh 266 Gujarat Chandigarh Madhya Pradesh Himachal Pradesh Bihar Manipur 261 Mizoram Andaman Nicobar Islands Assam Jharkhand Arunachal Pradesh 251 Tripura 247 Meghalaya Telengana Nagaland Sikkim Dadra Nagar Haveli Daman 181 Cases being reassigned states Telangana Dadar Nagar Haveli Unassigned Telangana*** Maharashtra*** Telengana*** Chandigarh*** Daman Punjab*** Name: State, dtype: int64 Explanation: above example, have imported dataframe module dask library read_csv() function order read content from file. have then used name column "States" followed value_counts() method count total numbers each value present that specific column. result, state's names present that column with total number their occurrences. Example Using groupby function Dask dataframe import dask.dataframe my_ddfile ddf.read_csv("covid_19_india.csv") print(my_ddfile.groupby(my_ddfile.State).Cured.max().compute()) Output: State Andaman Nicobar Islands 4647 Andhra Pradesh 860368 Arunachal Pradesh 15690 Assam 209447 Bihar 232563 Cases being reassigned states Chandigarh 16981 Chandigarh*** 14381 Chhattisgarh 227158 Dadar Nagar Haveli Dadra Nagar Haveli Daman 3330 Daman Delhi 565039 46924 Gujarat 203111 Haryana 232108 Himachal Pradesh 37871 Jammu Kashmir 107282 Jharkhand 107898 Karnataka 858370 Kerala 582351 Ladakh 8056 Madhya Pradesh 200664 Maharashtra 1737080 Maharashtra*** 1581373 Manipur 23166 Meghalaya 11686 Mizoram 3772 Nagaland 10781 Odisha 316970 Puducherry 36308 Punjab 145093 Punjab*** 130406 Rajasthan 260773 Sikkim 4735 Tamil Nadu 770378 Telangana 41332 Telangana*** 40334 Telengana 266120 Telengana*** 42909 Tripura 32169 Unassigned Uttar Pradesh 528832 Uttarakhand 72435 West Bengal 475425 Name: Cured, dtype: int64 Explanation: above program, have again imported dataframe module dask library used read_csv order read from specified file. Then, have used groupby function max() function dask dataframe find number cured people from each state. Now, understand another Dask Interface that Dask Machine Learning. Dask Machine Learning Dask Machine Learning offers algorithms scalable machine learning Python, which compatible with scikit-learn. begin with understanding handling computations using scikit-learn then have closer look into Dask performs these functions different way. user execute parallel computing with help scikit-learn solitary system) placing parameter njobs Scikit-learn utilizes Joblib order execute these parallel computations. Joblib Python library that offers support parallelization. When call fit() function, based tasks executed (whether hyperparameter search fitting model), Joblib distributes task across cores available. However, scale parallel computation perform with help scikit-learn library multiple machines. Whereas, Dask performs well solitary system well easily scaled cluster systems. Dask provides central task scheduler group workers. scheduler assigns tasks each worker. Then these workers assigned number cores which they execute computations. workers deliver functions: Compute assigned tasks Serve results other workers request. consider example demonstrating conversation between scheduler workers (This example been provided developer Dask, namely Matthew Rocklin): Central Task Scheduler sends work form python functions workers execute either same system cluster one. Worker please calculate f(1), Worker please calculate g(2) Worker once g(2) function complete, please from Worker perform h(x, above example should provide clear demonstration about working Dask. understand models machine learning Dask-search Machine Learning Models Dask Machine Learning (also known Dask-ML) offers scalable machine learning Python. before started, follow Dask-ML installation steps given below: Installation using conda conda install conda-forge dask-ml Installation using install dask-ml move onto understanding Parallelizing Scikit-Learn directly reimplementing Algorithms using Dask Array. Parallelizing Scikit-Learn Directly have already discussed, Scikit-Learn (also known sklearn) offers parallel computing single CPU) with help Joblib. directly utilize Dask order parallelize more than sklearn estimators inserting lines code (without even making modifications current code). primary step import client from distributed module dask library. This command will generate local schedule worker system. from dask.distributed import Client starting local Dask client my_client Client() next step instantiate joblib dask backend. have import parallel_backend from joblib sklearn library shown following syntax: import dask_ml.joblib from sklearn.externals.joblib import parallel_backend with parallel_backend('dask'): Normal scikit-learn code goes here from sklearn.ensemble import RandomForestClassifier my_model RandomForestClassifier() Reimplementing Algorithms using Dask Array Dask-ML reimplements simple Machine Learning Algorithms order NumPy Arrays. NumPy arrays replaced Dask using Dask Arrays order achieve Scalable Algorithms. This replacement helps implement: Linear Models (For example, Linear Regression, Poisson Regression, Logistic Regression, etc.) Pre-Processing (For example, Scalers, Transforms, etc.) Clustering (For example, K-means, Spectral Clustering, etc.) Linear model example from dask_ml.linear_model import LogisticRegression mymodel LogisticRegression() mymodel.fit(data, labels) Pre-processing example from dask_ml.preprocessing import OneHotEncoder myencoder OneHotEncoder(sparse=True) myresult myencoder.fit(data) Clustering example from dask_ml.cluster import KMeans mymodel KMeans() mymodel.fit(data) Dask-Search Hyperparameter tuning considered significant step building model critically alter implementation model. Models Machine Learning have various hyperparameters, tough understand which parameter would perform better specific situation. Executing this task manually considerably tiresome work. However, Scikit-Learn library offers Gridsearch order simplify task hyperparameter tuning. user must provide parameters, Gridsearch will offer best combination these parameters. consider example where need pick random forest technique order dataset. model three significant tunable parameters First Parameter, Second Parameter, Third Parameter, respectively. Now, values these parameters below: First Parameter Bootstrap True Second Parameter max_depth Third Parameter n_estimators [50, 200] sklearn Gridsearch: every parameter combination, Scikit-learn Gridsearch will execute tasks, sometimes ending iterating single task multiple time. graph shown below, demonstrating that this exactly most effective method: Dask-Search contrast Gridsearch's sklearn, Dask offers library known Dask-Search Dask-Search merges steps order reduce repetitions. install Dask-search using step shown below: Installing Dask-Search using conda conda install dask-searchcv conda-forge Installing Dask-Search using install dask-searchcv Here graph shown below that demonstrates working Dask-Search Difference between Spark Dask Here difference between Spark Dask: Spark Dask Spark written Scala Programming Language. Dask written Python Programming Language. Spark offers support Python. Dask only supports Python. Spark provides ecosystem. Dask components Python ecosystem. Spark offers Application Programming Interfaces (APIs). Dask reutilizes Application Programming Interfaces (APIs) Pandas Spark easy understand implement Scala users. Dask normally preferred Python Practitioners. Spark does include support Multi-dimensional Arrays natively. Dask provides full support NumPy models scalable Multi-dimensional Arrays. Next TopicMenu-Driven Programs Python prev next