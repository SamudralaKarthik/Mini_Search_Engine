next prev Sklearn Model Selection Sklearn's model selection module provides various functions cross-validate model, tune estimator's hyperparameters, produce validation learning curves. Here list functions provided this module. Later will understand theory these functions with code examples. Splitter Classes model_selection.GroupKFold([ n_splits This function variant KFold cross-validation, which forms non-overlapping groups. model_selection.GroupShuffleSplit([ This function performs Shuffle-Group(s)-Out cross-validation test. model_selection.KFold([ n_splits, shuffle, This function used perform KFold cross-validation test. model_selection.LeaveOneGroupOut( This function performs Leave Group cross-validation test. model_selection.LeavePGroupsOut( n_groups This function performs test using Leave Group Out. model_selection.LeaveOneOut( This test performs Leave cross-validation. model_selection.LeavePOut( Cross-validator with Leave-One-Out model_selection.PredefinedSplit( test_fold This general version Leave test, i.e. Leave cross-validation test. model_selection.RepeatedKFold( n_splits, This performs cross-validation test predefined split. model_selection.RepeatedStratifiedKFold( This test performs repeated stratified K-Fold cross-validation. model_selection.ShuffleSplit([n_splits, This function performs cross-validation test shuffled slit dataset using random permutations. model_selection.StratifiedKFold([ n_splits, This function performs stratified KFold cross-validation test. model_selection.StratifiedShuffleSplit([ This function performs stratified shuffle split cross-validation test. model_selection.StratifiedGroupKFold([ This function performs stratified K-Folds cross-validation test non-overlapping groups. model_selection.TimeSeriesSplit([ n_splits, This cross-validation test time series. Splitter Functions model_selection.check_cv([ classifier This function checks utility perform cross-validation test. model_selection.train_test_split( *arrays[, ...] This function performs cross-validation separating matrices arrays into training testing datasets random. Hyper-parameter optimizers model_selection.GridSearchCV( estimator, This function executes exhaustive search estimator over defined parameters. model_selection.HalvingGridSearchCV( ...[, ...] This function executes search over given parameters using successive halving. model_selection.ParameterGrid( param_grid This function performs grid parameters, where each parameter discrete range values. model_selection.ParameterSampler( ...[, ...] This function acts generator parameter samples taken from given distribution. model_selection.RandomizedSearchCV( ...[, ...] This function performs randomized search hyper-parameters. model_selection.HalvingRandomSearchCV( ...[, ...] This function performs randomized search hyper-parameters. Cross-validation: assessing performance estimator fundamental error data scientists make while creating model learning parameters forecasting function evaluating model same dataset. model that simply repeats labels data points just been trained would score well unable make predictions about data that model seen. Overfitting term this circumstance. common reserve portion given data validation test test, test) when conducting machine learning experiment avoid this problem. should note that term "experiment" does just refer academic purposes since machine learning experiments sometimes begin today's business world. Code Python program show perform k-fold cross-validation test Importing required library from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score, KFold Loading dataset iris load_iris() Getting dependent independent features iris.data iris.target print("Size Dataset len( Creating logistic regression object logreg LogisticRegression() Performing logistic regression train dataset logreg.fit( Performing K-fold cross-validation test KFold(n_splits score cross_val_score(logreg, cv=kf) Printing accuracy scores print("K-fold Cross Validation Scores are: score) print("Mean Cross Validation score score.mean()) Output Size Dataset K-fold Cross Validation Scores are: 0.86666667 0.93333333 0.83333333] Mean Cross Validation score 0.9266666666666665 Another example cross-validation method. Code Python program show perform stratified k-fold cross-validation test Importing required library from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score, StratifiedKFold Loading dataset iris load_iris() Getting dependent independent features iris.data iris.target print("Size Dataset len(X)) Creating logistic regression object logreg LogisticRegression() Performing logistic regression train dataset logreg.fit(X, Python program show perform Leave cross-validation test Importing required library from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, LeavePOut Loading dataset iris load_iris() Getting dependent independent features iris.data iris.target print("Size Dataset len(X)) Splitting dataset LeavePOut cross-validation test leave_pout LeavePOut(p leave_pout.get_n_splits(X) Creating Random forest classifier object computing accuracy score tree RandomForestClassifier(n_estimators max_depth n_jobs score cross_val_score(tree, leave_pout) Printing accuracy scores print("LeavePOut Cross Validation Scores are: score) print("Average Cross Validation score score.mean()) Output Size Dataset LeavePOut Cross Validation Scores are: Average Cross Validation score 0.9494854586129754 Cross-validated Metrics Calculation Calling cross_val _score utility method estimator dataset most straightforward approach calculating performance cross-validation test. following example shows ways split data, develop model, test through cross-validation test calculate score five times (using various splits each time) measure accuracy score support vector machine model sklearn's iris dataset. Code Python program calculate accuracy score through cross_val_score function Importing required library from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split from sklearn import datasets from sklearn import svm Importing dataset datasets.load_iris(return_X_y True) X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.4, random_state Training Support Vector Classifier model svm.SVC(kernel 'linear', 1).fit(X_train, Y_train) Printing accuracy score model print(svc.score(X_test, Y_test)) Calculating accuracy score through cross_val_score scores cross_val_score(svc, print(scores) print(scores.mean()) Output 0.9666666666666667 [0.96666667 0.96666667 0.96666667 0.9800000000000001 Tuning Hyper-Parameters Estimator Hyper-parameters variables that estimators explicitly acquire, they supplied parameters constructor class estimator. Support Vector Classifier, common examples kernel gamma, alpha Lasso, etc. Searching hyper-parameter domain maximum cross-validation score feasible advised. this method optimise supplied parameter while building estimator. following function: estimator.get_params() obtain names current values parameters given estimator. components search are: estimator function (for example, regressor classifier, like sklearn.svm.SVC()), parameter field, search strategy sampling procedure, cross-validation strategy, scoring function. GridSearchCV uses methods: "fit" "score." estimator grid search supports following methods, also implements the. methods "score samples", "predict proba", "predict", "transform", "inverse transform", "decision function". Cross-validated grid search employed over parameter grid improve estimator's parameters which grid search strategies used. Code Python t=program show perform grid search Importing required libraries from sklearn.svm import from sklearn.datasets import load_iris from sklearn.model_selection import GridSearchCV Loading dataset data load_iris() Specifying parameters parameters {'kernel':('linear', 'poly'), 'C':[1, 20]} Initializing model SVC() Performing grid search GridSearchCV(svc, parameters) Giving grid search function data target values print( gs.fit(data.data, data.target) sorted(gs.cv_results_.keys()) Output GridSearchCV(estimator=SVC(), param_grid={'C': 20], 'kernel': ('linear', 'poly')}) ['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score'] improve efficiency GridSearchCV performing halving grid search algorithm. search approach begins analysing each option with small number elements before selecting best options repeatedly with progressively more elements. Code Python program show perform halving grid search algorithm Importing required libraries from sklearn.datasets import load_iris from sklearn.ensemble import RandomForestClassifier from sklearn.experimental import enable_halving_search_cv from sklearn.model_selection import HalvingGridSearchCV Loading dataset load_iris(return_X_y True) Creating Random forest classifier model object RandomForestClassifier(random_state Specifu=ying parameters parameters {"max_depth": None], "min_samples_split": [10, 15]} Performing grid search search HalvingGridSearchCV(rfc, parameters, resource 'n_estimators', max_resources random_state 10).fit( print( search.best_params_ Output {'max_depth' None, 'min_samples_split': 'n_estimators': Points Remember while Performing Parameter Search Defining Objective Metric parametric search default evaluates parameter configuration using estimator's scoring function. classification, metrics used sklearn.metrics.accuracy_score, regression analysis, sklearn.metrics.r2_score. Other scoring metrics more appropriate certain situation. Defining More than Metrics Evaluation specify multiple scoring metrics with GridSearchCV RandomizedSearchCV functions. Multimetric scoring methods given parameters either Python dictionary mapping name scorer scoring function and/or predetermined scorer name collection strings predetermined score names(s). Nested Estimators Parameter Spaces Using special <estimator>_<parameter> syntax, GridSearchCV RandomizedSearchCV enable looking over parameters nested estimators like Pipeline, VotingClassifier, CalibratedClassifierCV ColumnTransformer. Parallelism parameter search functions separately evaluate each parameter permutation each data subset. term jobs=-1 enables parallel processing calculations. Robustness Failure Some parameter selections could make accommodating more data subsets impossible. Even though algorithm might completely assess some parameter values, this will, default, fail whole search. algorithm will resist such failures error_score=0 =np.NaN) specified, generating warning assigning score value that subset NaN) finishing search. Calculating Prediction Quality three different APIs assess well model predicts future: Estimator scoring system: Estimation methods have scoring system that offers default grading standard subject they intended address. This covered each estimator's manual. Scoring Criteria: Cross-validation modelling evaluation methods intrinsic scoring system, such sklearn.model_selection.cross_val_score sklearn.model_selection.GridSearchCV. Metric Systems: particular uses, sklearn.metrics module contains routines measuring estimation error. Examples Model Evaluations Techniques functions such model_selection.GridSearch() model_selection.cross_val_score() provide scoring parameter define scoring method. Using sklearn.metrics Include Different Scoring Methods Code Python program evaluate model using cross_val_score scoring criteria with different scoring functions. Importing required libraries from sklearn.model_selection import cross_val_score from sklearn.datasets import load_iris from sklearn.svm import Loading dataset load_iris(return_X_y True) Executing Support Vector Classifier algorithm dataset SVC(random_state Calculating prediction score using cross_val_score scores cross_val_score(svc, scoring 'balanced_accuracy') print("Accuracy score scoring method 'balanced_accuracy' scores) Using different scoring method SVC() scores cross_val_score(svc, scoring 'f1_micro') print("Accuracy score scoring method 'f1_micro' \n", scores) Output Accuracy score scoring method 'balanced_accuracy' [0.95238095 0.95238095 0.95238095 0.95238095 Accuracy score scoring method 'f1_micro' [0.95454545 0.95238095 0.95238095 0.95238095 Additionally, Scikit-learn's GridSearchCV, RandomizedSearchCV, cross_validate support evaluation based multiple metrics simultaneously. Code Python program evaluate model using cross_validate scoring criteria with multiple scoring methods simultaneously. Importing required libraries from sklearn.model_selection import cross_validate from sklearn.datasets import load_iris from sklearn.svm import from sklearn.metrics import accuracy_score from sklearn.metrics import make_scorer Loading dataset load_iris(return_X_y True) Executing Support Vector Classifier algorithm dataset SVC(random_state Giving scoring methods list scoring ['accuracy', 'r2'] Evaluating model using cross_validate metric cv_scores cross_validate(svc, scoring scoring) Getting scores print(cv_results) Giving scoring methods dictionary scoring {'accuracy_score': make_scorer(accuracy_score), 'r2_score': 'r2'} cv_scores cross_validate(svc, scoring scoring) Getting scores print(cv_results) Output {'fit_time': array([0.00099993, 0.00099993, 0.0150001 0.00200009, 0.00199986, 0.00100017, 0.00100017]), 'score_time': array([0.00099993, 0.00099993, 0.00099993, 0.00200033, 0.00099993, 0.00099993, 0.00099993]), 'test_accuracy': array([0.95454545, 0.95238095, 0.95238095, 0.95238095, 'test_prec': array([0.9331307 0.92857143, 0.92857143, 0.92857143, {'fit_time': array([0.00099993, 0.00099993, 0.0150001 0.00200009, 0.00199986, 0.00100017, 0.00100017]), 'score_time': array([0.00099993, 0.00099993, 0.00099993, 0.00200033, 0.00099993, 0.00099993, 0.00099993]), 'test_accuracy': array([0.95454545, 0.95238095, 0.95238095, 0.95238095, 'test_prec': array([0.9331307 0.92857143, 0.92857143, 0.92857143, Implementing Scoring Object creating custom scoring object instead using function make scorer, build different scoring methods which will more adaptable model. callable must adhere process outlined following principles scorer: supports parameter calls (estimator, gives back floating point value supplying data that measures accuracy score model's predictions sample dataset regarding target variable straightforward routines evaluating estimation error given predicted values true values dataset also provided sklearn.metrics module: greater value score predicted values returned functions that finish "_score," better model smaller score value predicted values returned functions that _error _loss, better model greater_is_better argument False using make_scorer function create scorer object. Validation Curves Learning Curves Each estimator that build benefits disadvantages. Biases, variance errors, noise combine form generalisation error. estimator's bias represented average value error across many training datasets. estimator's variance reveals responsive been various training datasets. Noise characteristic data. Since biases variance inherent characteristics estimators, typically have choose learning algorithms tune hyperparameters minimise bias variance. Adding extra training dataset model another widely used technique lower estimator's variance. However, must only gather more training data estimate cannot well model actual function with slight variation complexity. Validation Curve require scoring system, such classifier accuracy function, evaluate model. Grid search other similar strategies that tune hyperparameter estimator with best score validation dataset correct choose numerous hyperparameters estimator. aware that validation accuracy score biased longer reliable indicator generalisation hyperparameters tuned based must calculate accuracy score different test obtain accurate approximation generalisation. Code Python program validation curves compare training validation scores #Importing required libraries import numpy from sklearn.datasets import load_iris from sklearn.model_selection import validation_curve, train_test_split from sklearn.linear_model import Lasso Loading dataset np.random.seed(0) load_iris(return_X_y=True) Shuffling dataset X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.4, random_state Using validation curve calculate score values training_scores, validation_scores validation_curve(Lasso(), X_train, Y_train, param_name "alpha", param_range np.logspace(-8, print(training_scores) print(validation_scores) Output [[0.93973174 0.9494471 0.94188484 0.9435467 0.94419797 0.94252983 0.94608803 0.9460067 0.9423957 0.94253182] [0.93944487 0.94914898 0.94164577 0.94328439 0.9439338 0.94224685 0.94584605 0.9457347 0.94212464 0.94227221] 0.97300409 0.87636051 0.95494321 0.9417682 0.92512841 0.93081973 0.91016821 0.91853245 0.94939447 0.9475685 0.96865934 0.8803329 0.95417121 0.9445263 0.92533163 0.93193911 0.90834795 0.91843293 0.95295644 0.94553797] [-0.01991239 -0.20576132 -0.09876543 -0.01991239 -0.09876543 -0.51981806 -0.02805836 -0.01991239 -0.01991239]] estimator will under-fitted values training score values validation scores both small. estimator overfitted value training scores higher than values validation scores. Still, scenario otherwise, estimator performs quite well. typically impossible have high values validation score training score. Learning Curve learning curve displays estimator's training validation scores various lengths training samples which given argument. method determine much additional training dataset might help estimator more susceptible bias variance errors. Look example below, where show learning curves Lasso classifier. Code Python program validation curves produce learning curve #Importing required libraries import numpy from sklearn.datasets import load_iris from sklearn.model_selection import learning_curve, train_test_split from sklearn.linear_model import Lasso Loading dataset np.random.seed(0) load_iris(return_X_y=True) Shuffling dataset X_train, X_test, Y_train, Y_test train_test_split(X, test_size 0.4, random_state Using validation curve calculate score values training_sizes, training_scores, validation_scores learning_curve(Lasso(), X_train, Y_train, train_sizes=[20, 81], print(training_scores) print(validation_scores) Output Output [[0.50150218 0.43690063 0.5154695 0.51452348 0.51452348 0.51452348 0.51452348 0.51452348 0.51452348 0.51452348] [0.48754102 0.46153892 0.50115233 0.47687368 0.53209581 0.4838794 0.4838794 0.4838794 0.4838794 0.4838794 [0.47761141 0.45787213 0.48397355 0.46741065 0.49652965 0.47942832 0.50579115 0.48615822 0.45051371 0.48083313]] [[0.43915574 0.33445944 0.38829399 0.50764841 0.45173949 0.15496657 0.40651899 0.48801649 0.56571766 0.4732054 [0.44653145 0.42970004 0.4145817 0.4872139 0.43139094 0.21609031 0.42580156 0.48481259 0.55030939 0.4521308 [0.43844149 0.40086229 0.43313405 0.46494005 0.38326539 0.23284754 0.46030724 0.48905027 0.51427695 0.42897388]] Next TopicStandardScaler Sklearn prev next