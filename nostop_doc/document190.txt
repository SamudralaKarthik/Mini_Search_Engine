next prev Arima Model Python Introduction Time Series Forecasting Sequence recording metric over constant time intervals known Time Series. Based frequency, Time Series classified into following categories: Yearly (For example, Annual Budget) Quarterly (For example, Expenses) Monthly (For example, Air Traffic) Weekly (For example, Sale Quantity) Daily (For instance, Weather) Hourly (For example, Stocks Price) Minutes wise (For example, Inbound Calls Call Centre) Seconds wise (For example, Traffic) Once done with Time Series Analysis, have forecast order predict future values that series will going take. However, what need forecasting? Since forecasting Time Series, such Sales Demand, often incredible commercial value, which increases need forecasting. Time Series Forecasting generally used many manufacturing companies drives primary business planning, procurement, production activities. forecasts' errors will undulate throughout chain supply business framework, that stuff. Thus, significant order accurate predictions saving costs, critical success. Concepts Techniques behind Time Series forecasting also applied business, including manufacturing. Time Series forecasting broadly classified into categories: Univariate Time Series Forecasting: Univariate Time Series Forecasting forecasting time series where utilize former values time series only order guess forthcoming values. Multi-Variate Time Series Forecasting: Multi-Variate Time Series Forecasting forecasting time series where utilize predictors other than series, also known exogenous variables, order forecast. following tutorial, will understand specific type method known ARIMA modeling. Auto Regressive Integrated Moving Average, abbreviated ARIMA, Algorithm forecasting that centered concept that data previous values time series alone utilized order predict future values. understand ARIMA Models detail. Introduction ARIMA Models ARIMA, abbreviated 'Auto Regressive Integrated Moving Average', class models that 'demonstrates' given time series based previous values: lags lagged errors forecasting, that equation utilized order forecast future values. model Time Series that non-seasons exhibiting patterns random white noise with ARIMA models. There three terms characterizing ARIMA model: where, order term order term number differences required make time series stationary Time Series seasonal patterns, have insert seasonal periods, becomes SARIMA, short 'Seasonal ARIMA'. Now, before understanding "the order term", discuss term. What 'p', 'q', ARIMA model? primary step make time series stationary order build ARIMA model. This because term 'Auto Regressive' ARIMA implies Linear Regression Model using lags predictors. already know, Linear Regression Models work well independent non-correlated predictors. order make series stationary, will utilize most common approach that subtract past value from present value. Sometimes, depending series complexity, multiple subtractions required. Therefore, value minimum number subtractions required make series stationary. time series already stationary, thus becomes Now, understand terms 'q'. order 'AR' (Auto-Regressive) term, which means that number lags utilized predictors. same time, 'q' order 'MA' (Moving Average) term, which means that number lagged forecast errors should used ARIMA Model. Now, understand what 'AR' 'MA' models detail. Understanding Auto-Regressive (AR) Moving Average (MA) Models following section, will discuss models actual mathematical formula these models. Pure (Auto-Regressive only) Model model which relies only lags. Hence, also conclude that function 'lags Yt' where, Yt-1 lag1 series. β1 coefficient lag1 term intercept that calculated model. Similarly, Pure (Moving Average only) model model where relies only lagged predicted errors. Where, error terms models errors corresponding lags. errors ϵt ϵt-1 errors from equations given below: Thus, have concluded Auto-Regressive (AR) Moving Average (MA) models, respectively. understand equation ARIMA Model. ARIMA model model where series time subtracted least once order make stationary, combine Auto-Regressive (AR) Moving Average (MA) terms. Hence, following equation: ARIMA Model words: Forecasted Constant Linear Combination Lags lags) Linear Combination Lagged Predicted Errors lags) Thus, objective this model find values However, find one? begin with finding ARIMA Model. Finding order differencing ARIMA Model primary purpose differencing ARIMA model make Time Series stationary. However, have take care over-differencing series over-differenced series also stationary, which will affect parameter model later. Now, understand appropriate differencing order. most appropriate differencing order minimum differencing needed order achieve almost stationary series roaming around defined mean plat reaching Zero relatively faster. case autocorrelations positive multiple lags (generally, more), series requires further differencing. contrast, autocorrelated itself pretty negatively, then series possibly over-differenced. cases where cannot actually decide between differencing orders, then have choose order providing minor standard deviation differenced series. consider example check series stationary. will Augmented Dickey-Fuller Test (adfuller()) from statsmodels package Python Programming Language. Example: from statsmodels.tsa.stattools import adfuller from numpy import import pandas mydata pd.read_csv('mydataset.csv', names ['value'], header adfuller( mydata.value.dropna()) print('Augmented Dickey-Fuller Statistic: res[0]) print('p-value: res[1]) Output: Augmented Dickey-Fuller Statistic: -2.464240 p-value: 0.124419 Explanation: above example, have imported adfuller module along with numpy's module pandas. have then used pandas library read file. have then used adfuller method printed values user. necessary check whether series stationary not. not, have difference; else, becomes zero. Augmented Dickey-Fuller (ADF) test's null hypothesis that time series stationary. Thus, test's p-value less than significance level (0.05), then will reject null hypothesis infer that time series definitely stationary. observe, p-value more significant than level significance. Therefore, difference series check plot autocorrelation shown below. Example: import numpy pandas from statsmodels.graphics.tsaplots import plot_acf, plot_pacf import matplotlib.pyplot plt.rcParams.update({'figure.figsize' (9,7), 'figure.dpi' 120}) Importing data pd.read_csv('mydataset.csv', names ['value'], header Genuine Series fig, axes plt.subplots(3, sharex True) axes[0, 0].plot(df.value); axes[0, 0].set_title('The Genuine Series') plot_acf(df.value, axes[0, Order Differencing: First axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('Order Differencing: First') plot_acf(df.value.diff().dropna(), axes[1, Order Differencing: Second axes[2, 0].plot(df.value.diff().diff()); axes[2, 0].set_title('Order Differencing: Second') plot_acf(df.value.diff().diff().dropna(), axes[2, plt.show() Output: Explanation: above example, have imported required libraries modules. have then imported data plot different graphs. have plotted original series graph, first-order differencing, second-order differencing along with their autocorrelation graphs. observe, time series reached stationarity with differencing orders. However, when have look plot autocorrelation Second order differencing, going into negative zone pretty faster, indicating series might have been over differenced. Hence, will tentatively fixing differencing order because series properly stationary, that series weak stationarity. This done shown below. Example: from pmdarima.arima.utils import ndiffs import pandas pd.read_csv('mydataset.csv', names ['value'], header df.value Augmented Dickey Fuller Test adftest ndiffs(X, test 'adf') KPSS Test kpsstest ndiffs(X, test 'kpss') Test pptest ndiffs(X, test 'pp') print("ADF Test adftest) print("KPSS Test kpsstest) print("PP Test pptest) Output: Test KPSS Test Test Explanation: above example, have imported ndiffs method pmdarima module. have then imported dataset defined object containing values from dataset. used ndiffs method perform ADF, KPSS, Tests printed their results users. Finding order Auto-Regressive (AR) term (p) following section, will discuss steps check whether model requires Auto-Regressive (AR) terms. number terms needed found studying Partial Autocorrelation (PACF) plot. consider Partial Autocorrelation correlation between series once exclude contributions from intermediate lags. Thus, PACF tends convey pure correlation between series lag. Hence, identify whether that required Auto-Regressive (AR) term not. Partial Autocorrelation lag(k) series coefficient that Auto-Regression Equation Now, understand find number terms? know, Autocorrelation stationary series rectified inserting enough terms. Thus, initially take order Auto-Regressive (AR) term equivalent many lags that cross limit significance PACF Plot. Example: import numpy pandas from statsmodels.graphics.tsaplots import plot_acf, plot_pacf import matplotlib.pyplot plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120}) importing data pd.read_csv('mydataset.csv', names ['value'], header fig, axes plt.subplots(1, sharex True) axes[0].plot(df.value.diff()); axes[0].set_title('Order Differencing: First') axes[1].set(ylim (0,5)) plot_pacf(df.value.diff().dropna(), axes[1]) plt.show() Output: Explanation: above example, have imported required libraries, modules, datasets. have then plotted graphs represent First Order Differencing partial autocorrelation. result, observe that PACF pretty significant above line significance. Lag also appears substantial, entirely maintaining cross limit significance (blue region). However, will conservative tentatively. Finding Order Moving Average (MA) term (q) Similar what have looked earlier PACF plot number Auto-Regressive (AR) Terms, plot find number Moving Average (MA) Terms. Moving Average (MA) term theoretically, lagged forecast's error. plot expresses number Moving Average (MA) terms needed remove autocorrelation stationary series. consider following example understanding autocorrelation plot differenced series. Example: import numpy pandas from statsmodels.graphics.tsaplots import plot_acf, plot_pacf import matplotlib.pyplot plt.rcParams.update({'figure.figsize' (9,3), 'figure.dpi' 120}) importing data mydata pd.read_csv('mydataset.csv', names ['value'], header fig, axes plt.subplots(1, sharex True) axes[0].plot(mydata.value.diff()); axes[0].set_title('Order Differencing: First') axes[1].set(ylim 1.2)) plot_acf(mydata.value.diff().dropna(), axes[1]) plt.show() Output: Explanation: above example, have imported required libraries, modules, datasets. have then plotted graphs represent First Order Differencing Autocorrelation. result, observe that some lags pretty above line significance. tentatively. also simpler model case doubt that adequately demonstrates Handling Slightly Under Over-Differenced Time Series Sometimes, situation occur where series slightly under-differenced, differencing time extra makes series somewhat over-differenced. such cases, have multiple additional Auto-Regressive (AR) Terms slightly under-differenced Time Series extra Moving Average (MA) Term slightly over-differenced Time Series. Once have discussed most topics, begin creating ARIMA Model Time Series Forecasting. Building ARIMA Model Once have determined values will creating ARIMA model. implementation ARIMA() module shown below: Example: import numpy pandas from statsmodels.tsa.arima_model import ARIMA importing data mydata pd.read_csv('mydataset.csv', names ['value'], header Creating ARIMA model mymodel ARIMA(mydata.value, order modelfit mymodel.fit(disp print(modelfit.summary()) Output: ARIMA Model Results ============================================================================== Dep. Variable: D.value Observations: Model: ARIMA(1, Likelihood -253.790 Method: css-mle S.D. innovations 3.119 Date: Thu, 2021 517.579 Time: 21:10:37 530.555 Sample: HQIC 522.829 ================================================================================= coef P>|z| [0.025 0.975] --------------------------------------------------------------------------------- const 1.1202 1.290 0.868 0.385 -1.409 3.649 ar.L1.D.value 0.6351 0.257 2.469 0.014 0.131 1.139 ma.L1.D.value 0.5287 0.355 1.489 0.136 -0.167 1.224 ma.L2.D.value -0.0010 0.321 -0.003 0.998 -0.631 0.629 Roots ============================================================================= Real Imaginary Modulus Frequency ----------------------------------------------------------------------------- AR.1 1.5746 +0.0000j 1.5746 0.0000 MA.1 -1.8850 +0.0000j 1.8850 0.5000 MA.2 545.5472 +0.0000j 545.5472 0.0000 ----------------------------------------------------------------------------- Explanation: above example, have imported module called ARIMA from statsmodels class create ARIMA model order have then printed summary model user. observe, overview model reveals details. middle table table coefficients where 'coef' values related terms' weights. also notice that term's coefficient tends zero, P-Value 'P |z|' column exceedingly insignificant. P-Value should less than 0.05, ideally corresponding significant. Now, rebuilding model without term. Example: import numpy pandas from statsmodels.tsa.arima_model import ARIMA importing data mydata pd.read_csv('mydataset.csv', names ['value'], header Creating ARIMA model mymodel ARIMA(mydata.value, order modelfit mymodel.fit(disp print(modelfit.summary()) Output: ARIMA Model Results ============================================================================== Dep. Variable: D.value Observations: Model: ARIMA(1, Likelihood -253.790 Method: css-mle S.D. innovations 3.119 Date: Thu, 2021 515.579 Time: 21:34:00 525.960 Sample: HQIC 519.779 ================================================================================= coef P>|z| [0.025 0.975] --------------------------------------------------------------------------------- const 1.1205 1.286 0.871 0.384 -1.400 3.641 ar.L1.D.value 0.6344 0.087 7.317 0.000 0.464 0.804 ma.L1.D.value 0.5297 0.089 5.932 0.000 0.355 0.705 Roots ============================================================================= Real Imaginary Modulus Frequency ----------------------------------------------------------------------------- AR.1 1.5764 +0.0000j 1.5764 0.0000 MA.1 -1.8879 +0.0000j 1.8879 0.5000 ----------------------------------------------------------------------------- Explanation: above example, have reduced model, which actually good. also observe that AR1 MA1 terms' P-Values' have been improved highly significant (<< 0.05). Now, plot residuals order ensure that there patterns such constant mean variance. Example: import numpy pandas from statsmodels.tsa.arima_model import ARIMA import matplotlib.pyplot plt.rcParams.update({'figure.figsize' (9,3), 'figure.dpi' 120}) importing data mydata pd.read_csv('mydataset.csv', names ['value'], header Creating ARIMA model mymodel ARIMA(mydata.value, order modelfit mymodel.fit(disp Plotting Residual Errors myresiduals pd.DataFrame(modelfit.resid) fig, plt.subplots(1,2) myresiduals.plot(title "Residuals", ax[0]) myresiduals.plot(kind 'kde', title 'Density', ax[1]) plt.show() Output: Explanation: above example, have plotted residual errors density graphs. observe that residual errors look fair with around zero mean uniform variance. plot graph representing actuals fitted values with help plot_predict() function. Example: import numpy pandas from statsmodels.tsa.arima_model import ARIMA import matplotlib.pyplot plt.rcParams.update({'figure.figsize' (9,3), 'figure.dpi' 120}) importing data mydata pd.read_csv('mydataset.csv', names ['value'], header Creating ARIMA model mymodel ARIMA(mydata.value, order modelfit mymodel.fit(disp Actual Fitted modelfit.plot_predict(dynamic False) plt.show() Output: Explanation: above example, have plotted 'actuals fitted' graph dynamic False. result, in-sample lagged values utilized forecasting. Thus, model gets trained until past value makes following forecast. Therefore, create fitted forecast, actuals appear preciously delicate. Next TopicPython Modulus Operator prev next