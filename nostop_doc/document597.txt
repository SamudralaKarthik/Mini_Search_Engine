next prev Q-Learning Python Reinforcement learning model Learning Process which learning agent develops, over time best possible within particular environment, engaging continuously with surroundings. During journey learning, agent will encounter different scenarios surroundings it's They known states. agent state select from variety permissible actions, which result various rewards punishments). agent learning over time develops ability maximize these rewards perform optimally condition Q-Learning fundamental type reinforcement learning that utilizes Q-values (also known action values) improve learner's behaviour continuously. Q-Values, also known Action-Values:Q-values defined actions states. estimate probability performing action time estimation computed iteratively using TD-Update rule that will learn about coming sections. Episodes Rewards:An agent, throughout life, begins state beginning makes numerous shifts between present state next state, according type actions environment interacts with. Every step transition, agent state transition takes action, rewarded surrounding environment then goes state. event that, some point time, agent lands ending states, means that there more transitions that feasible. This referred episode. Temporal Difference TD-Update: temporal Difference (TD) Update rule could expressed following manner: Q(S,A)←Q(S,A)+ α(R+ γQ(S`,A`)-Q(S,A)) update rule used calculate Quantity used each time stage Agent's interaction with their environment. terminology used explained below: S: current state Agent. current Action selected policy. S`: Next State where Agent will A`: next most effective option choose based most current Q-value estimation, i.e., select Action that highest Q-value following state. R: Current Reward seen environment response current actions. γ(>0 <=1): Discounting Factor Future Rewards. Future rewards lesser value than present rewards. Therefore they should discounted. Because Q-value estimates anticipated rewards from specific state, discounting rules also applicable this case. α: length step revise Q(S, A). Making Action undertake using ϵ-gracious policy: ϵ-greedy strategy simple method selecting actions based most current estimations Q-values. policy according following: With probability ϵ), pick option with most Q-value. With high probability (ϵ), pick choice random. With knowledge needed, let's example. will utilize environment created OpenAI build Q-Learning algorithm. Install gym: install gym, using following command: !pip3 install Before beginning with this example, will need helper code process algorithm. helper files need downloaded from working directory. Step Import required libraries modules. import GYM import itertools import matplotlib MPLOT import matplotlib.style MPLOTS import numpy import pandas import from collections import defaultdict Step will instantiate environment. gym.make("FrozenLake-v1") n_observations1 env.observation_space.n n_actions1 env.action_space.n Step have create initialize Q-table createEpsilonGreedyPolicy1(Q1, epsilon1, num_actions1): 	""" 	Here, will create epsilon-greedy policy which based given Q-function epsilon. will return function which will takes state 	as input then will return probabilities 	for each every action form numpy array length action space (Set possible actions). 	""" 	def policyFunction1(state): 		Action_probabilities1 nmp.ones(num_actions1, 				dtype float) epsilon1 num_actions1 				 		best_action nmp.argmax(Q1[state]) 		Action_probabilities[best_action] (1.0 epsilon1) 		return Action_probabilities1 	return policyFunction1 Step will build Q-Learning Model. qLearning1(env, num_episodes1, discount_factor 1.0, 							alpha 0.6, epsilon1 0.1): 	""" 	The Q-Learning algorithm: Off-policy control. used finding optimal greedy policy while improving epsilon-greedy policy""" This will Action value function, which nested dictionary that maps state (action action-value). 	Q1 DD(lambda: nmp.zeros(env.action_space.n)) Keeps track useful statistics 	stats PLOTT.EpisodeStats( 		episode_lengths nmp.zeros(num_episodes1), 		episode_rewards nmp.zeros(num_episodes1))	 Here, will creating epsilon greedy policy function which would #appropriate environment action space 	policy createEpsilonGreedyPolicy1(Q1, epsilon1, env.action_space.n) each every episode 	for Kth_episode range(num_episodes1): Here, will resetting environment #then will picking first action 		state env.reset() 		for itertools.count(): here, will getting probabilities actions from current #state 			action_probabilities1 policy(state) Now, will choosing action according probability distribution 			action nmp.random.choice(nmp.arange( 					len(action_probabilities1)), 					p action_probabilities1) 			# Now, will taking action getting reward transit next state 			next_state, reward, done, env.step(action) 			# Now, will updating statistics 			stats.episode_rewards[Kth_episode] reward 			stats.episode_lengths[Kth_episode] 			# Update 			best_next_action nmp.argmax(Q1[next_state])	 			td_target reward discount_factor Q1[next_state][best_next_action] 			td_delta td_target Q1[state][action] 			Q1[state][action] alpha td_delta 			# Now, here done True episode terminated 			if done: 				break 				 			state next_state 	return stats Step will train model. stats qLearning(env, 1000) Step last, will Plot important statistics. PLOTT.plot_episode_stats(stats) Output: Conclusion Episode Reward Over Time plot that rewards each episode gradually increasing time until gets point high reward episode, which suggests that this agent learned maximize total reward each episode exhibiting optimal behaviour each every level. Next TopicCombinatoric Iterators Python prev next