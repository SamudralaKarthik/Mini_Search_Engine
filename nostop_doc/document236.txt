next prev Gradient Descent Algorithm Gradient Descent optimization algorithm which used minimize cost function many machine learning algorithms. Gradient Descent algorithm used updating parameters learning models. Following different types Gradient Descent: Batch Gradient Descent: Batch Gradient Descent type Gradient Algorithm that used processing training datasets each iteration gradient descent. Suppose number training dataset large, batch gradient descent will comparatively expensive. Hence, number training dataset large, users advised batch gradient descent. Instead, they mini-batch gradient descent large training dataset. Mini-Batch Gradient Descent: mini-batch gradient descent type gradient descent that used working faster than other types gradient descent. Suppose user (where batch gradient descent) dataset where (where mini-batch gradient descent) will processed iteration. even number training dataset large, mini-batch gradient descent will process batches training datasets single attempt. Therefore, work large training datasets with fewer numbers iterations. Stochastic Gradient Descent: Stochastic gradient descent type gradient descent which process training dataset iteration. Therefore, parameters will updated after each iteration, which only dataset been processed. This type gradient descent faster than Batch Gradient Descent. But, number training datasets large then also, will process only dataset time. Therefore, number iterations will large. Variables used: number training datasets. 'j' number features dataset. mini-batch gradient descent will behave similarly batch gradient descent. (where batch gradient descent) Algorithm used batch gradient descent: hθ(a) hypothesis linear regression. Then cost function will given represents training datasets from Gtrain(θ) (1/2k) (hθ(a(t)) b(t))2 Repeat (learning rate/k) (hθ(a(t)) b(t))ag(t) every Where ag(t) represents gth feature training dataset, suppose very large (for example, million number training datasets), then batch gradient descent will take hours maybe days completing process. Therefore, large training datasets, batch gradient descent recommended users this will slows down learning process machine. Algorithm used mini-batch gradient descent Suppose number datasets batch, where 100; However users adjust batch size. This generally written power Repeat ….., summation from t+9 represented (learning rate/size (p)) (hθ(a(d)) b(d))ag(d) every Algorithm used stochastic gradient descent: This gradient descent will randomly shuffle dataset training parameters each kind data. stochastic gradient descent will take only dataset iteration. Hence, (a(t), b(t)) training dataset Cost(θ, (a(t), b(t))) (1/2) (hθ(a(t)) b(t))2 Gtrain(θ) (1/k) Cost (θ, (a(t), b(t))) Repeat k{ (learning rate) (hθ(a(t)) b(t))ag(t) every Conclusion this tutorial, have discussed about different types Gradient Descent algorithms their variants. Next TopicPrettytable Python prev next