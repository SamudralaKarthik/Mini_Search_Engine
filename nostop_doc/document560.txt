next prev Implementation Linear Regression using Python Linear regression statistical technique describe relationships between dependent variables with number independent variables. This tutorial will discuss basic concepts linear regression well application within Python. order give understanding basics concept linear regression, begin with most basic form linear regression, i.e., "Simple linear regression". Simple Linear Regression Simple linear regression (SLR) method predict response using feature. believed that both variables linearly linked. Thus, strive find linear equation that predict answer value(y) precisely possible relation features independently derived variable(x). Let's consider dataset which have number responses feature simplification, define: feature vector, i.e., [x1, x3, xn], response vector, i.e., [y1, yn] observations above example, 10). scatter plot above dataset looks like: next step identify line that most suitable this scatter graph that anticipate response value feature. (i.e., value that dataset) This line referred regression line. equation regression line shown follows: Here, h(xi signifies predicted response value ?1xi regression coefficients represent y-intercept slope regression line respectively. order build model, need "learn" estimate value regression coefficients After we've determined those coefficients, then able make this model order forecast response! this tutorial, we're going employ concept Least Squares. Let's consider: ?0+ ?1xi ?i=h(xi )+ ?i= yi- h(xi Here, residual error observation. goal minimize total residual error. have defined cost function squared error, mission find value ?1 which J(?0,?1) minimum. Without going into mathematical details, presenting result below: Where, ssxy would cross deviations "x": ssxx would squared deviations Code: import numpy import matplotlib.pyplot mtplt estimate_coeff(p, Here, will estimate total number points observation 	n1 nmp.size(p) Now, will calculate mean vector 	m_p nmp.mean(p) 	m_q nmp.mean(q) here, will calculate cross deviation deviation about 	SS_pq nmp.sum(q 	SS_pp nmp.sum(p here, will calculate regression coefficients 	b_1 SS_pq SS_pp 	b_0 	return (b_0, b_1) plot_regression_line(p, Now, will plot actual points observation scatter plot 	mtplt.scatter(p, color "m", 			marker "o", here, will calculate predicted response vector 	q_pred b[0] b[1] here, will plot regression line 	mtplt.plot(p, q_pred, color "g") here, will labels 	mtplt.xlabel('p') 	mtplt.ylabel('q') here, will define function show plot 	mtplt.show() main(): entering observation points data 	p np.array([10, 19]) 	q np.array([11, 22]) now, will estimate coefficients 	b estimate_coeff(p, 	print("Estimated coefficients :\nb_0 		\nb_1 {}".format(b[0], b[1])) Now, will plot regression line 	plot_regression_line(p, __name__ "__main__": 	main() Output: Estimated coefficients b_0 -0.4606060606060609 1.1696969696969697 Multiple linear regression Multiple linear regression attempts explain relationships among several elements then respond applying linear equation with data. Clearly, it's anything more than extension linear regression. Imagine data that more features independent variables) well response dependent variable). dataset also comprised additional rows/observations. define: X(Feature Matrix) matrix size "n p" where "xij" represents values attribute observation. Therefore, And, (Response Vector) vector size where represents value response observation. regression line features represented Where h(xi) predicted response value observation point ?0,?1,?2,....,?p regression coefficients. also write: Where, representing residual error observation point. also generalize linear model little more representing attribute matrix Therefore, linear model expressed terms matrices shown below: y=X?+? Where, determine estimation i.e., with algorithm called Least Squares method. previously mentioned, this Least Squares method used find case where residual error total minimized. will present results shown below: Where matrix's transpose matrix's that reverse. With help lowest square estimates b', multi-linear regression model calculated where estimated response vector. Code: import matplotlib.pyplot mtpplt import numpy from sklearn import datasets from sklearn import linear_model LM from sklearn import metrics mts First, will load boston dataset boston1 DS.load_boston(return_X_y False) Here, will define feature matrix(H) response vector(f) boston1.data boston1.target Now, will split datasets into training testing sets from sklearn.model_selection import train_test_split H_train, H_test, f_train, f_test tts(H, test_size 0.4, 													random_state Here, will create linear regression object reg1 LM.LinearRegression() Now, will train model using training sets reg1.fit(H_train, f_train) here, will print regression coefficients print('Regression Coefficients are: reg1.coef_) Here, will print variance score: means perfect prediction print('Variance score {}'.format(reg1.score(H_test, f_test))) Here, will plot residual error here, will plot style mtpplt.style.use('fivethirtyeight') here will plot residual errors training data mtpplt.scatter(reg1.predict(H_train), reg1.predict(H_train) f_train, 			color "green", label 'Train data') Here, will plot residual errors test data mtpplt.scatter(reg1.predict(H_test), reg1.predict(H_test) f_test, 			color "blue", label 'Test data') Here, will plot line zero residual error mtpplt.hlines(y xmin xmax linewidth here, will plot legend mtpplt.legend(loc 'upper right') now, will plot title mtpplt.title("Residual errors") here, will define method call showing plot mtpplt.show() Output: Regression Coefficients are: [-8.95714048e-02 6.73132853e-02 5.04649248e-02 2.18579583e+00 -1.72053975e+01 3.63606995e+00 2.05579939e-03 -1.36602886e+00 2.89576718e-01 -1.22700072e-02 -8.34881849e-01 9.40360790e-03 -5.04008320e-01] Variance score 0.7209056672661751 above example, calculate accuracy score using Explained Variance Score. define: explained_variance_score Var{y y'}/Var{y} where estimated output target, where equivalent (correct) target's output, where Variance, which square standard deviation. most ideal score 1.0. Lower scores worse. Assumptions: Here main assumptions that linear regression model based with regard dataset basis which utilized: Linear relationships: Relationship between feature response variables must linear. assumption linearity tested with help scatter plots. see, figure representation linearly related variables, whereas variables figures probably non-linear. Thus, figure make more accurate predictions using linear regression. Little multi-collinearity: assumption that there minimal multicollinearity present data. Multi-collinearity happens when features independent variables) aren't independent each other. Little auto-correlation: Another theory could that there's much autocorrelation within data. Autocorrelation when residual errors aren't independent another. Homoscedasticity: refer instance where errors factor (that "noise" random disturbance relationship between independent variables dependent variables) that remains same independent variables. Figure homoscedastic, while figure displays heteroscedasticity. tutorial, we'll discuss some applications linear regression. Applications: Following fields applications based Linear Regression: Trend lines: illustrate changes quantity data passing time (like prices, instance.). They typically have linear connection. Therefore, linear regression could used predict future value. However, method able meet requirements scientific credibility situations when other possible changes alter data. Economics: Linear Regression primary tool used economics. employed predict amount spending consumers fixed investment spending investments inventory, purchases exports country, spending imports, demand store liquid assets, demand labour, supply. Financial: model capital value assets makes linear regression study quantify risk factors investing. Biology: Linear regression method explain causal relationships between variables biological systems. Next TopicNested Decorators Python prev next