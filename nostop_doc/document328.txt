next prev Principal Component Analysis (PCA) with Python Principal Component Analysis (PCA): algebraic technique converting observations possibly correlated variables into values liner uncorrelated variables. principal components chosen describe most available variance variable, principal components orthogonal each other. sets principal component first principal component will always have maximum variance. Different Uses Principal Component Analysis: used finding interrelations between various variables data. used interpreting visualizing data sets. also used visualizing genetic distance connection between populations. also makes analysis simple with decrease number variables. Principal component analysations usually executed square symmetric matrix, this pure squares cross products matrix correlation matrix covariance matrix. correlation matrix used there major difference individual variance. What Objectives Principal Component Analysis? basic objectives follows: nondependent method used reducing attribute space from larger number variables smaller number factors. dimension reducing technique with assurance whether dimension would interpretable. PCA, main selecting subset variables from larger set, depending which original variables will have highest correlation with principal amount. Principal Axis Method: Principal Component Analysis searches linear combination variable extracting maximum variance from variables. Once done with process, will move forward another linear combination which will explain maximum ratio remaining variance, which would lead orthogonal factors sets. This method used analysing total variance variables set. Eigen Vector: nonzero vector that remains parallel after multiplying matrix. Suppose 'V' eigen vector dimension matrix with dimension parallel. Then user solve where both unknown solving eigen vector eigen value. Eigen Value: also known "characteristic roots" PCA. This used measuring variance variables set, which reported that factor. proportion eigen value ratio descriptive importance factors concerning variables. factor low, then subsidises less description variables. Now, will Discuss Principal Component Analysis with Python. Following Steps Using with Python: this tutorial, will wine.csv Dataset. Step will import libraries. import numpy import matplotlib.pyplot mpltl import pandas Step will import dataset (wine.csv) First, will import dataset distribute into components data analysis. pnd.read_csv('Wine.csv') Now, will distribute dataset into components DS.iloc[: 0:13].values DS.iloc[: 13].values Step this step, will split dataset into training testing set. from sklearn.model_selection import train_test_split X_train, X_test, Y_train, Y_test tts(X, test_size 0.2, random_state Step Now, will Feature Scaling. this step, will re-processing training testing set, example, fitting standard scale. from sklearn.preprocessing import StandardScaler SC SS() X_train SC.fit_transform(X_train) X_test SC.transform(X_test) Step Then, Apply function will apply function into training testing analysis. from sklearn.decomposition import PCa (n_components X_train PCa.fit_transform(X_train) X_test PCa.transform(X_test) explained_variance PCa.explained_variance_ratio_ Step Now, will Logistic Regression training from sklearn.linear_model import LogisticRegression classifier_1 (random_state classifier_1.fit(X_train, Y_train) Output: LogisticRegression(random_state=0) Step Here, will predict testing result: Y_pred classifier_1.predict(X_test) Step will create confusion matrix. from sklearn.metrics import confusion_matrix c_m (Y_test, Y_pred) Step Then, predict result training set. from matplotlib.colors import ListedColormap X_set, Y_set X_train, Y_train X_1, nmp.meshgrid(nmp.arange(start X_set[:, 0].min() stop X_set[: 0].max() step 0.01), nmp.arange(start X_set[: 1].min() stop X_set[: 1].max() step 0.01)) mpltl.contourf(X_1, X_2, classifier_1.predict(nmp.array([X_1.ravel(), X_2.ravel()]).T).reshape(X_1.shape), alpha 0.75, cmap (('yellow', 'grey', 'green'))) mpltl.xlim (X_1.min(), X_1.max()) mpltl.ylim (X_2.min(), X_2.max()) enumerate(nmp.unique(Y_set)): mpltl.scatter(X_set[Y_set X_set[Y_set (('red', 'green', 'blue'))(s), label mpltl.title('Logistic Regression Training set: mpltl.xlabel ('PC_1') X_label mpltl.ylabel ('PC_2') Y_label mpltl.legend() showing legend show scatter plot mpltl.show() Output: Step last, will visualize result testing set. from matplotlib.colors import ListedColormap X_set, Y_set X_test, Y_test X_1, nmp.meshgrid(nmp.arange(start X_set[: 0].min() stop X_set[: 0].max() step 0.01), nmp.arange(start X_set[: 1].min() stop X_set[: 1].max() step 0.01)) mpltl.contourf(X_1, X_2, classifier_1.predict(nmp.array([X_1.ravel(), X_2.ravel()]).T).reshape(X_1.shape), alpha 0.75, cmap LCM(('pink', 'grey', 'aquamarine'))) mpltl.xlim(X_1.min(), X_1.max()) mpltl.ylim(X_2.min(), X_2.max()) enumerate(nmp.unique(Y_set)): mpltl.scatter(X_set[Y_set X_set[Y_set LCM(('red', 'green', 'blue'))(s), label title scatter plot mpltl.title('Logistic Regression Testing set') mpltl.xlabel ('PC_1') X_label mpltl.ylabel ('PC_2') Y_label mpltl.legend() show scatter plot mpltl.show() Output: Conclusion this tutorial, have learned about principal component analysis with Python, uses, objects data analyse data's testing training sets. Next TopicPython Program Find Number Days Between Given Dates prev next